{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#FOR-BETTER-EXPERIENCE\" data-toc-modified-id=\"FOR-BETTER-EXPERIENCE-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>FOR BETTER EXPERIENCE</a></span></li><li><span><a href=\"#TABLE-OF-CONTENT\" data-toc-modified-id=\"TABLE-OF-CONTENT-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TABLE OF CONTENT</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Brazilian E-commerce Olist Sentiment Analysis** <a class=\"tocSkip\">\n",
    "*Created by : Alfian & Owen Agitza Jaya* <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">FOR BETTER EXPERIENCE</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please run cell code below***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Larger cell code\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notes***\n",
    "* Hyperlink that I made might not work well with several nbextension like `table of content`\n",
    "* Others might also causing the issue, but I think uncheck only `table of content` will resolve the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">TABLE OF CONTENT</h1>\n",
    "\n",
    "<a id=\"toc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0. PREFACE](#0)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[0.1. LIBRARY](#0-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[0.2. USER-DEFINED FUNCTIONS](#0-2)<br><br>\n",
    "[1. BUSINESS UNDERSTANDING](#1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1. CONTEXT](#1-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2. PROBLEM STATEMENT](#1-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.3. OBJECTIVES](#1-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.4. ANALYTIC APPROACH](#1-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.5. EVALUATION METRIC](#1-5)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.6. REFERENCES](#1-6)<br><br>\n",
    "[2. DATA](#2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.1. DATASET INFORMATION](#2-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2. DATASET PREVIEW](#2-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1. GEOLOCATION DATASET](#2-2-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2. CUSTOMERS DATASET](#2-2-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3. ORDER ITEMS DATASET](#2-2-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.4. PAYMENTS DATASET](#2-2-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.5. REVIEWS DATASET](#2-2-5)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.6. ORDERS DATASET](#2-2-6)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.7. PRODUCTS DATASET](#2-2-7)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.2.8. SELLERS DATASET](#2-2-8)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3. DATASET PROCESSING](#2-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1. REVIEWS DATASET (MAIN)](#2-3-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.4. COMBINING DATASETS](#2-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.4.1. TABLE 1. ORDERS-REVIEWS-ORDER ITEMS](#2-4-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[2.4.2. TABLE 2. ORDERS-REVIEWS-CUSTOMERS-ORDER ITEMS-SELLERS](#2-4-2)<br><br>\n",
    "[3. EXPLORATORY DATA ANALYSIS](#3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.1. DATA FRAME PROCESSING](#3-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1. HOW'S THE ORDER DENSITY ACROSS STATES? HOW'S THEIR SENTIMENT PROPORTIONS?](#3-1-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2. HOW'S THE OVERALL ORDER REVIEWS AT A GLANCE? IS THERE ANY DIFFERENCE IN REVIEW SCORE BETWEEN COMMENT AND NO COMMENT ORDERS?](#3-1-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3. HOW'S THE DIFFERENCE OF DELIVERY PATH BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS?](#3-1-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.4. HOW'S THE DISTRIBUTION OF DELIVERY DATES BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS? IS IT STATISTICALLY DIFFERENT?](#3-1-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.5. HOW'S SELLER STATE'S PERFORMANCE IN TERMS OF DELIVERY DAY DIFFERENCE?](#3-1-5)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.2. DATA VISUALIZATION AND STATISTICS](#3-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1. HOW'S THE ORDER DENSITY ACROSS STATES? HOW'S THEIR SENTIMENT PROPORTIONS?](#3-2-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2. HOW'S THE OVERALL ORDER REVIEWS AT A GLANCE? IS THERE ANY DIFFERENCE IN REVIEW SCORE BETWEEN COMMENT AND NO COMMENT ORDERS?](#3-2-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.3. HOW'S THE DIFFERENCE OF DELIVERY PATH BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS?](#3-2-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.4. HOW'S THE DISTRIBUTION OF DELIVERY DATES BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS? IS IT STATISTICALLY DIFFERENT?](#3-2-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.5. HOW'S SELLER STATE'S PERFORMANCE IN TERMS OF DELIVERY DAY DIFFERENCE?](#3-2-5)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.3. TABLEAU DASHBOARD](#3-3)<br><br>\n",
    "[4. MACHINE LEARNING](#4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.1. DATA PREPARATION](#4-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.2. TEXT PREPROCESSING](#4-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.3. SENTIMENT CONTENT & LENGTH OF SENTENCE](#4-3)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4. BASE MODEL EVALUATION](#4-4)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.5. BASE MODEL EVALUATION WITH OVERSAMPLING (SMOTE)](#4-5)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.6. BASE MODEL EVALUATION WITH UNDERSAMPLING (NEARMISS)](#4-6)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.7. BASE MODEL OUTPUT RECAP](#4-7)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.8. HYPERPARAMETER TUNING - LOGISTIC REGRESSION](#4-8)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.9. FEATURE IMPORTANCE](#4-9)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.10. IMPLEMENTATION](#4-10)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.11. LEARNING CURVE](#4-11)<br><br>\n",
    "[5. CONCLUSION & RECOMMENDATION](#5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>0. PREFACE</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>0.1. LIBRARY</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.9.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googletrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01memoji\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Ignore warning\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googletrans'"
     ]
    }
   ],
   "source": [
    "# Data frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set_palette(sns.color_palette('deep'))\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly as py\n",
    "py.offline.init_notebook_mode(connected = True)\n",
    "from itertools import chain, cycle\n",
    "from IPython.display import display_html\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Data cleaning\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from operator import attrgetter\n",
    "\n",
    "# EDA\n",
    "from scipy.stats import mannwhitneyu, shapiro\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import operator\n",
    "import functools\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from googletrans import Translator\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ignore warning\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Machine Learning\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, recall_score, precision_score, average_precision_score, accuracy_score, precision_recall_curve, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Resampling Method\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "# Set max columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Show all columns on pandas dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Show all value in columns, wider columns\n",
    "pd.set_option(\"max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>0.2. USER-DEFINED FUNCTIONS</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a data frame\n",
    "def preview(df):\n",
    "    \"\"\"Receives data frame.\n",
    "    Returns a summary of attributes including dtype, proportion of missing values,\n",
    "    number of unique values, and sample.\"\"\"\n",
    "    \n",
    "    dfCheck= []\n",
    "    for i in df.columns:\n",
    "        dfCheck.append([\n",
    "                    i,\n",
    "                    df[i].dtype,\n",
    "                    df[i].isna().sum(),\n",
    "                    round(df[i].isna().sum() / len(df) * 100, 2),\n",
    "                    df[i].nunique(),\n",
    "                    df[i].drop_duplicates().values])\n",
    "        \n",
    "    return pd.DataFrame(dfCheck, columns = ['Features', 'DataType', 'Null', 'Proportion', 'Uniques', 'UniqueSample'])\n",
    "\n",
    "# Outlier\n",
    "def outlier (df, numerical):\n",
    "    \"\"\"Receives data frame and numerical columns.\n",
    "    Returns a list of all records of outliers index.\"\"\"\n",
    "    \n",
    "    q1 = df[numerical].quantile(0.25)\n",
    "    q2 = df[numerical].quantile(0.5)\n",
    "    q3 = df[numerical].quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    o = df[(df[numerical] < (q1 - (1.5 * iqr))) | (df[numerical] > (q3 + (1.5 * iqr)))].index\n",
    "    \n",
    "    return df.loc[o]\n",
    "\n",
    "# Transforms document to lower case\n",
    "def to_lower(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns the text in lower case.\"\"\"\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "contractions_dict = {     \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"}\n",
    "\n",
    "# Using contractions_dict to expand contractions.\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    \"\"\"Receives a text and a contraction dictionary.\n",
    "    Returns expanded text without contractions.\"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags = re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text\n",
    "\n",
    "# Subfunction to expand_contractions\n",
    "def main_contraction(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns expanded text without contractions.\"\"\"\n",
    "    \n",
    "    text = expand_contractions(text, contractions_dict)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Removes the number in a document\n",
    "def remove_numbers(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns text without numbers.\"\"\"\n",
    "    \n",
    "    output = ''.join(c for c in text if not c.isdigit())\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Removes punctuations in a document\n",
    "def remove_punct(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns text without punctuations.\"\"\"\n",
    "    \n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "# Remove punctuations in a document 2\n",
    "# Why modify into remove_punct2?  Because we found a case in which {text1.text2} became text1text2.\n",
    "# We don't want that to happen. so replace punctuation with \" \", then later remove whitespace with defined function to_strip\n",
    "def remove_punct2(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns text without punctuations.\"\"\"\n",
    "    \n",
    "    combined_doc = []\n",
    "    for c in text : \n",
    "        if c not in punctuation:\n",
    "            combined_doc.append(c)\n",
    "        if c in punctuation:\n",
    "            combined_doc.append(\" \")\n",
    "\n",
    "    return \"\".join(combined_doc)\n",
    "\n",
    "# Removes unnecessary whitespace\n",
    "def to_strip(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns text without whitespace.\"\"\"\n",
    "    \n",
    "    return \" \".join(text.split())\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download stopwords, stopwords basically unimportant information like ...\n",
    "stop_words_mod_en = (stopwords.words('english') + ['yet', 'would', 'year'])\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"Receives a sentence.\n",
    "    Returns sentence without stopwords.\"\"\"\n",
    "    \n",
    "    stop_words = stop_words_mod_en\n",
    "    \n",
    "    return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
    "\n",
    "# Necessary library to stemming the document\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Transform words to their basic component\n",
    "def stem(text):\n",
    "    \"\"\"Receives text.\n",
    "    Returns the text in its basic component.\"\"\"\n",
    "    \n",
    "    stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    return \" \".join(stemmed_word)\n",
    "\n",
    "#  Necessary library to lemmatize the document\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Transform words to their basic component, but based on vocabulary in the dictionary\n",
    "def lemmatize(text):\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    return \" \".join(lemmatized_word)\n",
    "\n",
    "# Makes a dictionary from corpus\n",
    "def dictionary(check):\n",
    "    \"\"\"Receives a text from the corpus.\n",
    "    Returns dictionary from the corpus.\"\"\"\n",
    "    \n",
    "    check = check.str.extractall(\"([a-zA_Z]+)\")\n",
    "    check.columns = ['check']\n",
    "    b = check.reset_index(drop = True)\n",
    "    check = b['check'].value_counts()\n",
    "\n",
    "    dictionary = {'word' : check.index, 'freq' : check.values}\n",
    "    dictionary = pd.DataFrame(dictionary)\n",
    "    dictionary.index = dictionary['word']\n",
    "    dictionary.drop('word', axis = 1, inplace = True)\n",
    "    dictionary.sort_values('freq', ascending = False, inplace = True)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "# Plot wordcloud to visualize what words show in the sentiment class\n",
    "def plot_cloud(wordcloud):\n",
    "    \"\"\"Receives a wordcloud object.\n",
    "    Returns a wordcloud visualization.\"\"\"\n",
    "    \n",
    "    # Set figure size\n",
    "    plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    # Display image\n",
    "    plt.imshow(wordcloud)\n",
    "    \n",
    "    # No axis details\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Extract emojis\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns a text without emojis.\"\"\"\n",
    "    \n",
    "    return ''.join(emj for emj in text if emj in emoji.UNICODE_EMOJI[\"en\"])\n",
    "\n",
    "# Splits emojis from each other\n",
    "# (https://stackoverflow.com/questions/49921720/how-to-split-emoji-from-each-other-python)\n",
    "def split_emojis(text):\n",
    "    \"\"\"Receives a text.\n",
    "    Returns a list of emojis inside the text.\"\"\"\n",
    "    \n",
    "    emoji_list = []\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "\n",
    "    for separated in em_split:\n",
    "        emoji_list.append(separated)\n",
    "        \n",
    "    return emoji_list\n",
    "\n",
    "# Replaces emojis\n",
    "def replace_emoticon(cols):\n",
    "    \"\"\"Receives a text column.\n",
    "    Returns a text with certain emoticons replaced with regular words.\"\"\"\n",
    "    \n",
    "    # Make A list of emoji with positive sentiment and negative sentiment \n",
    "    positive_sentiment_emojis = [\"😚\", \"😘\", \"😁\", \"😀\", \"😚\", \"😆\", \"👧\", \"😉\", \"😄\", \"😃\", \"😎\", \"🤗\", \"😍\", \"😊\", \"😂\",\n",
    "                                \"💯\", \"💚\", \"💖\", \"💟\", \"💓\", \"💝\", \"💋\", \"💕\", \"💞\", \"🌟\", \"🔝\", \n",
    "                                \"👊🏽\", \"👊\", \n",
    "                                \"👐\", \"🙌\", \"🙌🏻\",\n",
    "                                \"👌\", \"👌🏻\", \"👌🏼\", \"👍🏻\", \"👍\", \"👍🏼\", \"👍🏽\", \n",
    "                                \"👏\", \"👏🏻\", \"👏🏼\", \"👏🏽\", \"👏🏾\", \n",
    "                                \"🤙\", \"🤙🏼\", \"🤙🏻\"]\n",
    "\n",
    "    negative_sentiment_emojis = [\"😡\", \"😕\", \"😣\", \"😠\", \"😓\", \"🤔\", \"😒\", \"😩\", \"😭\", \"😟\", \"😞\", \n",
    "                                \"😥\", \"😔\",\n",
    "                                \"😤\", \n",
    "                                \"👎\", \"👎🏻\", \"🙏\", \"🙏🏻\"]\n",
    "    \n",
    "    ambiguous_sentiment_emojis = [\"😐\", \"😏\", \"😱\",\n",
    "                                  \"👆\", \"👉\", \"🏇\", \"📺\", \"💅\", \"🍀\", \"🌷\", \"🐘\", \"🎁\", \"🚚\", \"🐕\"]\n",
    "\n",
    "    # First we need to split the document into list of words & emoji\n",
    "    splitted_doc = split_emojis(cols['review_comment_message_en'])\n",
    "\n",
    "    # Make empty list to contain the words and modified emoji\n",
    "    words_list = []\n",
    "    \n",
    "    # Second we replace the emoji with our chosen words, for positive sentiment : approved, for negative sentiment : dissatisfied\n",
    "    for words in splitted_doc:\n",
    "        \n",
    "        # If the emoji in positive_sentiment_emojis, the emoji will be converted into satisfied\n",
    "        if words in positive_sentiment_emojis :\n",
    "            words_list.append('satisfied')\n",
    "            \n",
    "        # If the emoji in positive_sentiment_emojis, the emoji will be converted into satisfied\n",
    "        elif words in negative_sentiment_emojis :\n",
    "            words_list.append('dissatisfied')\n",
    " \n",
    "        # If the emoji in ambiguous_sentiment_emojis, the emoji will be converted into nothing\n",
    "        elif words in ambiguous_sentiment_emojis :\n",
    "            words_list.append(\"\")\n",
    "            \n",
    "        # If not in both list, we assume that it's not an emoji and directly add it to the list\n",
    "        else :\n",
    "            words_list.append(words)\n",
    "\n",
    "    # After that we return the modified document\n",
    "    return \" \".join(words_list)\n",
    "\n",
    "# Replaces emojis 2\n",
    "def replace_emoticon_for_output(text):\n",
    "    \"\"\"Receives a text column.\n",
    "    Returns a text with certain emoticons replaced with regular words.\"\"\"\n",
    "    \n",
    "    positive_sentiment_emojis = [\"😚\", \"😘\", \"😁\", \"😀\", \"😚\", \"😆\", \"👧\", \"😉\", \"😄\", \"😃\", \"😎\", \"🤗\", \"😍\", \"😊\", \"😂\",\n",
    "                                \"💯\", \"💚\", \"💖\", \"💟\", \"💓\", \"💝\", \"💋\", \"💕\", \"💞\", \"🌟\", \"🔝\", \n",
    "                                \"👊🏽\", \"👊\", \n",
    "                                \"👐\", \"🙌\", \"🙌🏻\",\n",
    "                                \"👌\", \"👌🏻\", \"👌🏼\", \"👍🏻\", \"👍\", \"👍🏼\", \"👍🏽\", \n",
    "                                \"👏\", \"👏🏻\", \"👏🏼\", \"👏🏽\", \"👏🏾\", \n",
    "                                \"🤙\", \"🤙🏼\", \"🤙🏻\"]\n",
    "\n",
    "    negative_sentiment_emojis = [\"😡\", \"😕\", \"😣\", \"😠\", \"😓\", \"🤔\", \"😒\", \"😩\", \"😭\", \"😟\", \"😞\", \n",
    "                                \"😥\", \"😔\",\n",
    "                                \"😤\", \n",
    "                                \"👎\", \"👎🏻\", \"🙏\", \"🙏🏻\"]\n",
    "\n",
    "    ambiguous_sentiment_emojis = [\"😐\", \"😏\", \"😱\",\n",
    "                                  \"👆\", \"👉\", \"🏇\", \"📺\", \"💅\", \"🍀\", \"🌷\", \"🐘\", \"🎁\", \"🚚\", \"🐕\"]\n",
    "\n",
    "    # First we need to split the document into list of words & emoji\n",
    "    splitted_doc = split_emojis(text)\n",
    "\n",
    "    # Make empty list to contain the words and modified emoji\n",
    "    words_list = []\n",
    "    \n",
    "    # Second we replace the emoji with our words of choosing, for positive sentiment : approved, for negative sentiment : dissatisfied\n",
    "    for words in splitted_doc:\n",
    "        \n",
    "        # If the emoji in positive_sentiment_emojis, the emoji will be converted into satisfied\n",
    "        if words in positive_sentiment_emojis :\n",
    "            words_list.append(\"satisfied\")\n",
    "            \n",
    "        # If the emoji in positive_sentiment_emojis, the emoji will be converted into satisfied\n",
    "        elif words in negative_sentiment_emojis :\n",
    "            words_list.append(\"dissatisfied\")\n",
    " \n",
    "        # If the emoji in ambiguous_sentiment_emojis, the emoji will be converted into nothing\n",
    "        elif words in ambiguous_sentiment_emojis :\n",
    "            words_list.append(\"\")\n",
    "            \n",
    "        # If not in the both list, we assume that it's not an emoji and directly add it to the list\n",
    "        else :\n",
    "            words_list.append(words)\n",
    "            \n",
    "    return \" \".join(words_list)\n",
    "\n",
    "# Constructs ngram count data frame\n",
    "def ngrams_count(corpus, ngram_range, n = 10, stopwords = stop_words_mod_en):\n",
    "    \"\"\"\n",
    "    Receives:\n",
    "    ----------\n",
    "    corpus: Text to be analysed [type: pd.DataFrame]\n",
    "    ngram_range: Type of n gram to be used on analysis [type: tuple]\n",
    "    n: Upper limit of ngrams to be shown [type: int, default: -1]\n",
    "    \n",
    "    Returns the result data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using CountVectorizer to build a bag of words using the given corpus\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords, ngram_range=ngram_range).fit(corpus)\n",
    "    \n",
    "    # Convert Corpus into DTM\n",
    "    bag_of_words = vectorizer.transform(corpus)\n",
    "    \n",
    "    # Calculate total sum of words\n",
    "    sum_words = bag_of_words.sum(axis = 0)\n",
    "    \n",
    "    # With forloop, connect the frequency and the feature names\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    # Sort the dictionary so the highest frequency will show as first list.\n",
    "    words_freq = sorted(words_freq, key = lambda x : x[1], reverse = True)\n",
    "    \n",
    "    # Only show n items.\n",
    "    total_list = words_freq[:n]\n",
    "    \n",
    "    # Returning a DataFrame with the ngrams count\n",
    "    count_df = pd.DataFrame(total_list, columns = ['ngram', 'count'])\n",
    "    \n",
    "    return count_df\n",
    "\n",
    "# Checks each feature categorical or numerical\n",
    "def col_info(df, i, figsize = (5, 3)):\n",
    "    \"\"\"Receives a data frame, index number, and figsize.\n",
    "    Returns a print of column infos.\"\"\"\n",
    "    \n",
    "    print(f\"Column index : {i}\")\n",
    "    print(f\"Column name : {df.columns[i]}\")\n",
    "    print(\"\\nUnique values in column\")\n",
    "    print(f\"{df[df.columns[i]].unique()}\")\n",
    "    print(\"\\nValue Counts\")\n",
    "    df_col_info = pd.DataFrame(df[df.columns[i]].value_counts())\n",
    "    df_col_info.rename(columns = {f\"{df.columns[i]}\" : f\"n_{df.columns[i]}\"}, inplace = True)\n",
    "    \n",
    "    plt.figure(figsize = figsize)\n",
    "    sns.countplot(data = df, x = df[df.columns[i]], palette = \"Set2\")\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_col_info\n",
    "\n",
    "# Displays data frame side by side\n",
    "def display_side_by_side(*args,titles = cycle([\"\"])):\n",
    "    \"\"\"Receives data frame args and its title\n",
    "    Returns a side by side display of those data frames.\"\"\"\n",
    "    \n",
    "    html_str = \"\"\n",
    "    for df,title in zip(args, chain(titles,cycle([\"</br>\"])) ):\n",
    "        html_str += \"<th style='text-align:center'><td style = 'vertical-align:top'>\"\n",
    "        html_str += f\"<h3>{title}</h3>\"\n",
    "        html_str += df.to_html().replace(\"table\",\"table style='display:inline'\")\n",
    "        html_str += \"</td></th>\"\n",
    "    display_html(html_str,raw=True)\n",
    "\n",
    "# Displays labels for model result compilation\n",
    "def labels(ax):\n",
    "    \"\"\"Receives an axis\n",
    "    Returns labels for the axis that is displayed inside plot.\"\"\"\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()                        # get bar length\n",
    "        ax.text(width,                               # set the text at 1 unit right of the bar\n",
    "                p.get_y() + p.get_height() / 2,      # get Y coordinate + X coordinate / 2\n",
    "                '{:1.4f}'.format(width),             # set variable to display, 2 decimals\n",
    "                ha = 'left',                         # horizontal alignment\n",
    "                va = 'center')                       # vertical alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes = None,\n",
    "    ylim = None,\n",
    "    cv = None,\n",
    "    n_jobs = None,\n",
    "    train_sizes = np.linspace(0.1, 1.0, 5),\n",
    "    scoring = 'roc_auc'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv = cv,\n",
    "        n_jobs = n_jobs,\n",
    "        train_sizes = train_sizes,\n",
    "        return_times = True,\n",
    "        scoring = scoring\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>1. BUSINESS UNDERSTANDING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.1. CONTEXT</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "[Olist](https://olist.com/pt-br/) is a Brazilian e-commerce **marketplace integrator** that operates in e-commerce sector [Azevedo, 2021]. It offers a marketplace solution to shopkeepers of all sizes to increase their sales whether they have online presence or not. As such, Olist connects small businesses to larger product marketplaces to help entrepreneurs sell their products to a larger customer base. The company was founded with the mission of helping small merchants gain market share across the country through a SaaS licensing model to small brick and mortar businesses.\n",
    "\n",
    "Olist differentiates itself from the other marketplaces based on reputation, product placements, dedicated teams to sales & customer service, pool of registered products, centralised control of operations, and other competitive tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "basewidth = 1600\n",
    "img = Image.open('Images/olist_jpg.jpg')\n",
    "wpercent = (basewidth / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.2. PROBLEM STATEMENT</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "Reviews are an integral part of any e-commerce business. They capture direct feedback from customers about products and most importantly, they are mostly freely given to the company. Hence, it is important to leverage this abundant information and create important signals to send feedback to the e-commerce system so that it can use them to further improve the customer experience. Moreover, reviews can be viewed by all customers, and they directly affect the sales of the products [Vajjala et al., 2020].\n",
    "\n",
    "The traditional way of garnering feedback for companies has been through surveys, closed user group testing, and so on. These methods have a considerable drawback including:\n",
    "- Manual methods of review analysis can be resource extensive in terms of time and money\n",
    "- Review analysis even more challenging by nature. They’re in the text and mostly in an unstructured format, full of unforced errors such as spelling mistakes, incorrect sentence constructions, incomplete words, and abbreviations\n",
    "\n",
    "As the frontline of e-commerce business in Brazil, Olist may find the automation of review analysis useful since Olist has a large database of user reviews at its disposal. One such utilization of reviews is **sentiment analysis**. Sentiment analysis has been one of the most active research areas in natural language processing since early 2000 . The aim of sentiment analysis is to define automatic tools able to extract subjective information from texts in natural language, such as opinions and sentiments, so as to create structured and actionable knowledge to be used by either a decision support system or a decision maker [Pozzi et al., 2016].\n",
    "\n",
    "Hence, this notebook tries to perform a sentiment analysis model that could sort through thousands of reviews and grasp the overall sentiment of customers in an instance. Furthermore, by the end of this notebook, the authors also wish to answer the following questions:\n",
    "1. What is the best modeling approach to be used for this sentiment analysis?\n",
    "    - What model performs the best?\n",
    "    - How does the best model's performance compare to the other models?\n",
    "1. How is the overall sentiment of customer reviews? What terms are found in both good and bad reviews?\n",
    "1. What recommended actions should be taken considering this information? How does it translate to actionable business decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.3. OBJECTIVES</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "The objectives of this notebook are: <br> \n",
    "> 1. Olist wants to develop a model that could sort through thousands of reviews and grasp the overall sentiment of customers in an instance\n",
    "> 1. Olist wants to know the overall sentiment of customer reviews and what terms are found in both good and bad reviews\n",
    "> 1. Olist wants to know the model impact and what actions should be taken given the result of the sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.4. ANALYTIC APPROACH</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "The problem defined above is categorized as a *classification* problem with no predetermined classes (labels). To overcome this absence of labels, the most ideal way is to manually label a portion of the reviews for the model to learn. However, due to time constraint the authors opted for an alternative solution which was to use a rule on the user reviews where:\n",
    "\n",
    "$$\\text{Rating} \\in [1,5] \\,\\, \\begin{cases} \\leq 3 \\Rightarrow \\text{Negative sentiment} \\\\ > 3 \\Rightarrow \\text{Positive sentiment} \\end{cases}$$\n",
    "\n",
    "It is important to notice that **the output of the model is not intended to predict rating classification, instead it is intended to predict the sentiment of the reviews**. The rule on the user reviews were merely as a creative approach for prelabeling purposes.\n",
    "\n",
    "> **Target** <br>\n",
    " 0 : Positive sentiment <br>\n",
    " 1 : Negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-5\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.5. EVALUATION METRIC</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "basewidth = 600\n",
    "img = Image.open('Images/evaluation_metric.png')\n",
    "wpercent = (basewidth / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* $TP$: True positive <br>\n",
    "Description: Negative sentiments that are correctly predicted as negative sentiments <br>\n",
    "\n",
    "* $FP$: False positive <br>\n",
    "Description: Positive sentiments that are incorrectly predicted as negative sentiments <br>\n",
    "Consequence: **Medium severity** <br>\n",
    "Comment: Misclassification of positive sentiments could underestimate the excellence done by Olist in terms of product management. This may discourage and induce confusion for future decisions regarding the current product lines. Furthermore, in the application of customer complain management, Olist may further delay the treatment of angry customers (Which give negative sentiments) since they wrongly classify positive sentiments as negatives\n",
    "\n",
    "* $TN$: True negative <br>\n",
    "Description: Positive sentiments that are correctly predicted as positive sentiments <br>\n",
    "\n",
    "* $FN$: False negative <br>\n",
    "Description: Negative sentiments that are incorrectly predicted as positive sentiments <br>\n",
    "Consequence: **Medium severity** <br> \n",
    "Comment: Misclassification of negative sentiments could exacerbate product management. Olist would underestimate the overall negative sentiments about their product and hence hindering future corrective actions for improvement. For instance, in the application of customer complain management, Olist may not be able to prioritize customers that give negative feedback (Angry) first. As such, this could exacerbate customer inconvenience as oppose to handling it\n",
    "***\n",
    "Based on the consequences, we want to mainly:\n",
    "1. Maximize $TP$ and $TN$ <br>\n",
    "1. Minimize $FP$ and $FN$ equally <br>\n",
    "\n",
    "This approach would give us a model that could prioritize correctly predicting both positive and negative sentiments. Furthermore, given the imbalanced nature of the dataset (shown in the next section), we chose **receiver operating characteristic (ROC) curve** to evaluate the probabilities model for this problem. The ROC curve [Fawcett, 2006] is a graphical evaluation method that is not dependent of a speciﬁc threshold. A ROC graph is a plot of `false positive rate` $(FPR)$ on the $x$-axis, and `true positive rate` $(TPR)$ on the $y$-axis. Specifically, we used the the area under the ROC curve (`ROC_AUC`) as it is the single metric that can summarize ROC curve. `ROC_AUC` is given by the formula [Fernández, 2018]:\n",
    "\n",
    "$$AUC_{ROC} = \\int_0^1TPR \\,\\, \\text{d}FPR$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN} \\\\[10pt]\n",
    "FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "As for the class labels evaluation, since FP and FN are equally costly, we chose to use **F1 score** (`f1`) as a secondary metric to measure the model's success in classifying labels. Its objective is to analyze the trade-offs between correctness and coverage in classifying positive instances [Baeza-Yates & Ribeiro-Neto, 1999]. The formula for `f1` is given as follows:\n",
    "\n",
    "$$F_1 = 2 \\times \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "\n",
    "This reasoning is based on the underlying business problem. Indeed, Olist's main motivation is to classify the maximum number of both positive and negative sentiments that means having a high value of $TP$ and $TN$. On the other hand, the misclassification of both positive and negative sentiments are equally detrimental for Olist as incorrect prediction in 1 class may affect the other just as much. Hence, it is preferred to prioritize a model that produces a high `ROC_AUC` first since it represents high proportion of $TP$ and $TN$ (i.e. the best model for the business case). Afterwards, the focus would be shifted in choosing a high threshold which produces the highest `f1` score, all of which could be achieved by using PR-curve & `ROC_AUC` as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1-6\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>1.6. REFERENCES</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azevedo, M.A., 2021. Goldman Sachs leads $23M in funding for Brazilian e-commerce startup Olist. TechCrunch. Available at: https://techcrunch.com/2021/04/15/goldman-sachs-leads-23m-in-funding-for-brazilian-e-commerce-startup-olist/ [Accessed June 13, 2022].\n",
    "\n",
    "Baeza-Yates, R. and Ribeiro-Neto, B., 1999. Modern information retrieval (Vol. 463). New York: ACM press.\n",
    "\n",
    "Fawcett, T., 2006. An introduction to ROC analysis. Pattern recognition letters, 27(8), pp.861-874.\n",
    "\n",
    "Fernández, A., García, S., Galar, M., Prati, R.C., Krawczyk, B. and Herrera, F., 2018. Learning from imbalanced data sets (Vol. 10, pp. 978-3). Berlin: Springer.\n",
    "\n",
    "Pozzi, F., Fersini, E., Messina, E. and Liu, B., 2016. Sentiment analysis in social networks. Morgan Kaufmann.\n",
    "\n",
    "Vajjala, S., Majumder, B., Gupta, A. and Surana, H., 2020. Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems. O'Reilly Media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>2. DATA</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>2.1. DATASET INFORMATION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "This dataset is a real commercial data of orders made at Olist. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. For completeness, it also includes a geolocation dataset that relates Brazilian zip codes to lat/lng coordinates.\n",
    "\n",
    "After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some reviews.\n",
    "\n",
    "**Attention**\n",
    "1. An order might have multiple items.\n",
    "1. Each item might be fulfilled by a distinct seller.\n",
    "1. All text identifying stores and partners where replaced by the names of Game of Thrones great houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "basewidth = 1200\n",
    "img = Image.open('Images/ERD_diagram.png')\n",
    "wpercent = (basewidth / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 datasets included in this study, all of which are connected with a primary key as shown in the figure above. The `olist_order_reviews_dataset` would be the primary dataset used for building sentiment analysis model.\n",
    "\n",
    "| No. | Dataset | Alias |\n",
    "| --- | --- | --- |\n",
    "| 1. | olist_geolocation_dataset | Geolocation dataset |\n",
    "| 2. | olist_order_customer_dataset | Customers dataset |\n",
    "| 3. | olist_order_items_dataset | Order items dataset |\n",
    "| 4. | olist_order_payments_dataset | Payments dataset |\n",
    "| 5. | olist_order_reviews_dataset | Reviews dataset (Main)|\n",
    "| 6. | olist_orders_dataset | Orders dataset |\n",
    "| 7. | olist_products_dataset | Products dataset |\n",
    "| 8. | olist_sellers_dataset | Sellers dataset |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>2.2. DATASET PREVIEW</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.1. GEOLOCATION DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset has information Brazilian zip codes and its lat/lng coordinates.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| geolocation_zip_code_prefix | Nominal | The first 5 digits of zip code |\n",
    "| geolocation_lat | Numerics | Latitude |\n",
    "| geolocation_lng | Numerics | Longitude |\n",
    "| geolocation_city | Nominal | City name |\n",
    "| geolocation_state | Nominal | State code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation = pd.read_csv('Dataset/olist_geolocation_dataset.csv')\n",
    "# Brazils most Northern spot is at 5 deg 16′ 27.8″ N latitude;\n",
    "geolocation = geolocation[geolocation.geolocation_lat <= 5.27438888]\n",
    "# It’s most Western spot is at 73 deg, 58′ 58.19″W Long.\n",
    "geolocation = geolocation[geolocation.geolocation_lng >= -73.98283055]\n",
    "# It’s most southern spot is at 33 deg, 45′ 04.21″ S Latitude.\n",
    "geolocation = geolocation[geolocation.geolocation_lat >= -33.75116944]\n",
    "# It’s most Eastern spot is 34 deg, 47′ 35.33″ W Long.\n",
    "geolocation = geolocation[geolocation.geolocation_lng <= -34.79314722]\n",
    "print(geolocation.shape)\n",
    "geolocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(geolocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.2. CUSTOMERS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset has information about the customer and its location.\n",
    "\n",
    "The Olist system assigns each order to a unique `customer_id`. This means that the same customer will get different IDs for different orders. The purpose of having a `customerunique_id` on the dataset is to allow Olist to identify customers that made repurchases at the store. Otherwise we would find that each order had a different customer associated with.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| customer_id | ID | The key to the orders dataset. Each order has a unique customer_id |\n",
    "| customer_unique_id | ID | Unique identifier of a customer |\n",
    "| customer_zip_code_prefix | Numerics | The first five digits of customer zip code |\n",
    "| customer_city | Nominal | Customer city name |\n",
    "| customer_state | Nominal | Customer state code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('Dataset/olist_order_customer_dataset.csv')\n",
    "print(customers.shape)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.3. ORDER ITEMS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset includes data about the items purchased within each order.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| order_id | ID | Order unique identifier |\n",
    "| order_item_id | Numerics | Sequential number identifying number of items included in the same order |\n",
    "| product_id | ID | Product unique identifier |\n",
    "| seller_id | ID | Seller unique identifier |\n",
    "| shipping_limit_date | Datetime | Shows the seller shipping limit date for handling the order over to the logistic partner |\n",
    "| price | Numerics | Item price (BRL) |\n",
    "| freight_value | Numerics | Item freight value item (if an order has more than one item the freight value is split between items) (BRL) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = pd.read_csv('Dataset/olist_order_items_dataset.csv')\n",
    "print(order_items.shape)\n",
    "order_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(order_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.4. PAYMENTS ITEMS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset includes data about the orders payment options.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| order_id | ID | Unique identifier of an order |\n",
    "| payment_sequential | Numerics | A customer may pay an order with more than one payment method. If he does so, a sequence will be created to keep track of it |\n",
    "| payment_type | Nominal | Method of payment chosen by the customer |\n",
    "| payment_installments | Numerics | Number of installments chosen by the customer |\n",
    "| payment_value | Numerics | Transaction value (BRL) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments = pd.read_csv('Dataset/olist_order_payments_dataset.csv')\n",
    "print(payments.shape)\n",
    "payments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-5\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.5. REVIEWS DATASET (MAIN)</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset includes data about the reviews made by the customers. This is the **main dataset** used for building sentiment analysis model.\n",
    "\n",
    "After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some reviews.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| review_id | ID | Unique review identifier |\n",
    "| order_id | ID | Unique order identifier |\n",
    "| review_score | Ordinal | Note ranging from 1 to 5 given by the customer on a satisfaction survey |\n",
    "| review_comment_title | Text | Comment title from the review left by the customer, in Portuguese |\n",
    "| review_comment_message | Text | Comment message from the review left by the customer, in Portuguese |\n",
    "| review_creation_date | Datetime | Shows the date in which the satisfaction survey is sent to the customer |\n",
    "| review_answer_timestamp | Datetime | Shows satisfaction survey answer timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('Dataset/olist_order_reviews_dataset.csv')\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-6\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.6. ORDERS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This is the core dataset. From each order we might find all other information.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| order_id | ID | Unique identifier of the order |\n",
    "| customer_id | ID | The key to the customer dataset. Each order has a unique customer_id |\n",
    "| order_status | Nominal | Reference to the order status (delivered, shipped, etc.) |\n",
    "| order_purchase_timestamp\t | Datetime | Shows the purchase timestamp |\n",
    "| order_approved_at | Datetime | Shows the payment approval timestamp | \n",
    "| order_delivered_carrier_date | Datetime | Shows the order posting timestamp when it is handled to the logistic partner |\n",
    "| order_delivered_customer_date | Datetime | Shows the actual order delivery date to the customer |\n",
    "| order_estimated_delivery_date | Datetime | Shows the estimated delivery date that is informed to customer at the purchase moment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('Dataset/olist_orders_dataset.csv')\n",
    "print(orders.shape)\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-7\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.7. PRODUCTS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset includes data about the products sold by Olist.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| product_id | ID | Unique product identifier |\n",
    "| product_category_name | Nominal | Root category of product, in Portuguese |\n",
    "| product_name_length | Numerics | Number of characters extracted from the product name |\n",
    "| product_description_length | Numerics | Number of characters extracted from the product description |\n",
    "| product_photos_qty | Numerics | Number of product published photos |\n",
    "| product_weight_g | Numerics | Product weight measured in grams |\n",
    "| product_length_cm | Numerics | Product length measured in centimeters |\n",
    "| product_height_cm | Numerics | Product height measured in centimeters |\n",
    "| product_width_cm | Numerics | Product width measured in centimeters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('Dataset/olist_products_dataset.csv')\n",
    "print(products.shape)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-2-8\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.2.8. SELLERS DATASET</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "This dataset includes data about the sellers that fulfilled orders made at Olist.\n",
    "\n",
    "| Attribute | Data type| Description |\n",
    "| --- | --- | --- |\n",
    "| seller_id | ID | Seller unique identifier |\n",
    "| seller_zip_code_prefix | Nominal | First 5 digits of seller zip code |\n",
    "| seller_city | Nominal | Seller city name |\n",
    "| seller_state | Nominal | Seller state code |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = pd.read_csv('Dataset/olist_sellers_dataset.csv')\n",
    "print(sellers.shape)\n",
    "sellers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(sellers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>2.3. DATASET PROCESSING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-3-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.3.1. REVIEWS DATASET (MAIN)</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, it could be seen that there is a severe missing value problem in `review_comment_title` and `review_comment_message` in `reviews` dataset. Since `review_comment_title` won't be used in modeling, we would just ignore it for now. As for the `review_comment_message`, since it is the main input for the sentiment analysis model, all missing values in this column would be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the best of our knowledge there's no imputing in NLP. Imputing can make sense in some cases with a numerical value (even then it should be used cautiously), but in general text is too diverse (unstructured data) for the concept of \"most frequent text\" to make any sense. In general substituting real text (or absence of text) with artificially generated data is frowned upon from the point of view of evaluation. Hence, as the main input for sentiment analysis model, all missing `review_comment_message` are omitted from the `reviews` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.dropna(axis = 0, subset = 'review_comment_message', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as stated in the analytic approach, due to time constraint the authors opted for an alternative solution to obtain class labels which was to use a rule on the user `review_comment_message` where:\n",
    "\n",
    "$$\\text{Rating} \\in [1,5] \\,\\, \\begin{cases} \\leq 3 \\Rightarrow \\text{Negative sentiment} \\,\\, (1) \\\\ > 3 \\Rightarrow \\text{Positive sentiment} \\,\\, (0) \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['class'] = np.where(reviews['review_score'] <= 3, 1, 0)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TARGET PROPORTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = reviews['class'].value_counts(normalize = True).reset_index()\n",
    "temp = temp.rename({'index': 'Sentiment class', 'Churn': 'Proportion'}, axis = 1)\n",
    "temp.set_index('Sentiment class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout = True, figsize = (12, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "explode = [0.01, 0.1]\n",
    "ax.pie(temp['class'], shadow = True, autopct = '%.2f%%', pctdistance = 0.5, colors = ['lightskyblue', 'lightcoral'], textprops = {'fontsize' : 14, 'color' : 'white'}, explode = explode, startangle = 60)\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "\n",
    "# Legend\n",
    "ax.legend(labels = ['Positive sentiment', 'Negative sentiment'], loc = 2, bbox_to_anchor = (1.01, 1))\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "\n",
    "# Title\n",
    "ax.set_title('Sentiment Class Proportions', size = 18, weight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it could be seen that the dataset is imbalanced in terms of the sentiment class proportions $(1:1.8)$, making it a mild imbalanced classification problem. Hence, the subsequent steps in the modeling should take this fact into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - The main input for the sentiment analysis, `review_comment_message`,  has about a severe case of missing values of $58.70\\%$. Despite this, we cannot just apply missiing values imputation because there is no such method in NLP\n",
    "> - Hence, we opted to drop all the rows with null values in `review_comment_message`\n",
    "> - The dataset is mildly imbalanced in terms of inferred sentiment class labels $(1:1.8)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>2.4. DATASET PROCESSING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-4-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.4.1. TABLE 1. ORDERS-REVIEWS-ORDER ITEMS</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table would be used whenever an inference or exploratory analysis involving only orders and reviews is conducted.\n",
    "\n",
    "Before merging tables, some duplicate values are dropped to maintain a coherent table. They are:\n",
    "1. `order_items` $\\Rightarrow$ Only `order_id` with the largest `order_item_id` is taken\n",
    "1. `reviews` $\\Rightarrow$ A single order can have multiple reviews, hence only the last review is considered for this merged table. In other words, `order_id` is dropped while keeping the last record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_temp = reviews.drop_duplicates(subset = ['order_id'], keep = 'last')\n",
    "print(reviews_temp.shape)\n",
    "reviews.head() # There can be multiple reviews on the same order_id => duplicate order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_temp = order_items.drop_duplicates(subset = ['order_id'], keep = 'last')\n",
    "print(order_items_temp)\n",
    "order_items.head() # There can be multiple orders containing more than 1 item => duplicate order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.merge(pd.merge(orders, reviews_temp, on = 'order_id', how = 'inner'), order_items_temp, on = 'order_id', how = 'inner')\n",
    "table1.drop(['product_id', 'seller_id'], axis = 1, inplace = True)\n",
    "new_cols = [col for col in table1.columns if col != 'class'] + ['class']\n",
    "table1 = table1[new_cols]\n",
    "print(table1.shape)\n",
    "table1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for any duplicated records in `table1` and did not find a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1[table1.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2-4-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>2.4.2. TABLE 2. ORDER-REVIEWS-CUSTOMERS-ORDER ITEMS=SELLERS</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table would be used whenever an inference or exploratory analysis involving customers and sellers is conducted.\n",
    "\n",
    "Before merging tables, some duplicate values are dropped to maintain a coherent table. They are:\n",
    "1. `reviews` $\\Rightarrow$ A single order can have multiple reviews, hence only the last review is considered for this merged table. In other words, `order_id` is dropped while keeping the last record\n",
    "1. `order_items` $\\Rightarrow$ Only `order_id` with the largest `order_item_id` is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_temp = reviews.drop_duplicates(subset = ['order_id'], keep = 'last')\n",
    "print(reviews_temp.shape)\n",
    "reviews.head() # There can be multiple reviews on the same order_id => duplicate order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orders.shape)\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers.shape)\n",
    "customers.head() # There can be multiple customer_ids refering to the same customer_unique_id => duplicate customer_unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_temp = order_items.drop_duplicates(subset = ['order_id'], keep = 'last')\n",
    "order_items.head() # There can be multiple orders containing more than 1 item => duplicate order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sellers.shape)\n",
    "sellers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell code below performs all the necessary data frame manipulations for `table2`. In short, it accounts for:\n",
    "1. Merging `orders`, `reviews`, `customers`, `order_items`, and `sellers`\n",
    "1. Changing all timestamps data into `pd.datetime`\n",
    "1. Extracting datetime attributes as new columns (year, month, year-month)\n",
    "1. Reordering the column sequence into a more logical order\n",
    "1. Dropping `order_purchase_year` of 2016 since the proportions are very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.merge(\n",
    "                  pd.merge(\n",
    "                           pd.merge(\n",
    "                                    pd.merge(orders, reviews_temp, on = 'order_id', how = 'inner'), \n",
    "                                    customers, on = 'customer_id', how = 'inner'), \n",
    "                           order_items_temp, on = 'order_id', how = 'inner'), \n",
    "                  sellers, on = 'seller_id', how = 'inner')\n",
    "\n",
    "# Changing the data type for timestamps into pd.datetime\n",
    "timestamp_cols = ['order_purchase_timestamp', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'shipping_limit_date']\n",
    "for col in timestamp_cols:\n",
    "    table2[col] = pd.to_datetime(table2[col])\n",
    "\n",
    "# Extracting attributes for purchase date\n",
    "table2['order_purchase_year'] = table2['order_purchase_timestamp'].apply(lambda x: x.year)\n",
    "table2['order_purchase_month'] = table2['order_purchase_timestamp'].apply(lambda x: x.month)\n",
    "table2['order_purchase_year_month'] = table2['order_purchase_timestamp'].apply(lambda x: x.strftime('%Y%m'))\n",
    "\n",
    "# Reordering columns 1\n",
    "moved_cols = ['order_purchase_year', 'order_purchase_month', 'order_purchase_year_month']\n",
    "before_col = table2.columns[:4].tolist()\n",
    "before_col.extend(moved_cols)\n",
    "new_cols = before_col + [col for col in table2.columns if col not in before_col]\n",
    "table2 = table2[new_cols]\n",
    "\n",
    "# Reordering columns 2\n",
    "new_cols = [col for col in table2.columns if col != 'class'] + ['class'] # Reordering the 'class' column to be at the end of the table\n",
    "table2 = table2[new_cols]\n",
    "\n",
    "# Dropping order_purchase_year == 2016 since the proportion is very small\n",
    "table2.drop(table2[table2['order_purchase_year'] == 2016].index, inplace = True)\n",
    "\n",
    "print(table2.shape)\n",
    "table2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check for any duplicated records in `table2` and did not find a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2[table2.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>3. EXPLORATORY DATA ANALYSIS</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>3.1. DATA FRAME PROCESSING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes all the processing needed to perform the data analysis. The subsections are grouped based on the question to be answered by the exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.1.1. HOW'S THE ORDER DENSITY ACROSS STATES? HOW'S THEIR SENTIMENT PROPORTIONS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we want to have a data frame that contains both customer and seller coordinates (latitude & longitude). So our approach was to merge a subset of `table2` and `geolocation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc = table2[['customer_zip_code_prefix', 'customer_state', 'seller_zip_code_prefix', 'seller_state', 'class']]\n",
    "stateloc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is similar to that of `VLOOKUP` from Excel, hence, we need to temporarily set the key (`customer_zip_code_prefix`) as the index of the data frame to be merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_temp1 = stateloc.set_index('customer_zip_code_prefix')\n",
    "stateloc_temp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_temp = geolocation.drop_duplicates(subset = 'geolocation_zip_code_prefix', keep = 'first')\n",
    "geolocation_temp = geolocation_temp.rename(columns = {'geolocation_zip_code_prefix' : 'customer_zip_code_prefix'})\n",
    "geolocation_temp = geolocation_temp.set_index('customer_zip_code_prefix')\n",
    "geolocation_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_temp2 = pd.merge(stateloc_temp1, geolocation_temp[['geolocation_lat', 'geolocation_lng']], left_index = True, right_index = True, how = 'inner')\n",
    "stateloc_temp2.reset_index(inplace = True)\n",
    "stateloc_temp2 = stateloc_temp2[['customer_zip_code_prefix', 'customer_state', 'geolocation_lat', 'geolocation_lng', 'seller_zip_code_prefix', 'seller_state', 'class']]\n",
    "stateloc_temp2 = stateloc_temp2.rename(columns = {'geolocation_lat' : 'customer_geolocation_lat', 'geolocation_lng' : 'customer_geolocation_lng'})\n",
    "stateloc_temp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach was done to `seller_zip_code_prefix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_temp3 = stateloc_temp2.set_index('seller_zip_code_prefix')\n",
    "stateloc_temp3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation_temp = geolocation.drop_duplicates(subset = 'geolocation_zip_code_prefix', keep = 'first')\n",
    "geolocation_temp = geolocation_temp.rename(columns = {'geolocation_zip_code_prefix' : 'seller_zip_code_prefix'})\n",
    "geolocation_temp = geolocation_temp.set_index('seller_zip_code_prefix')\n",
    "geolocation_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the desired data frame as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_final = pd.merge(stateloc_temp3, geolocation_temp[['geolocation_lat', 'geolocation_lng']], left_index = True, right_index = True, how = 'inner')\n",
    "stateloc_final.reset_index(inplace = True)\n",
    "stateloc_final = stateloc_final[['customer_zip_code_prefix', 'customer_state', 'customer_geolocation_lat', 'customer_geolocation_lng', 'seller_zip_code_prefix', 'seller_state', 'geolocation_lat', 'geolocation_lng', 'class']]\n",
    "stateloc_final = stateloc_final.rename(columns = {'geolocation_lat' : 'seller_geolocation_lat', 'geolocation_lng' : 'seller_geolocation_lng'})\n",
    "stateloc_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `stateloc_final`, we could derive the order count per state as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_oder_density = stateloc_final.groupby(['customer_state']).size().reset_index().rename(columns = {0 : 'count'})\n",
    "state_oder_density['proportion'] = state_oder_density['count'].apply(lambda x: x / state_oder_density['count'].sum() * 100).round(2)\n",
    "state_oder_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a list of northern states of Brazil for explaination purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "north = ['AM', 'AC', 'RR', 'AP', 'PA', 'MA', 'PI', 'CE', 'RN', 'PB', 'PE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to obtain a `geometry` object for every states in Brazil in order to create a proper choropleth map of Brazil. To do this, we used an external resource provided by the [Brazilian_Government](https://www.ibge.gov.br/). Then, we need to extract the data in the form of **geodataframe** with the appropriate `layer` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf = gpd.read_file('Resources/bcim_2016_21_11_2018.gpkg', layer = 'lim_unidade_federacao_a') # The parameter for `layer` is obtained from the source of geodataset\n",
    "print(f'The data type is {type(geodf)}')\n",
    "geodf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's left to do is to merge the geodataframe with `state_oder_density`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is IMPORTANT for geodf to be the left data frame, as it is a geodataframe (not dataframe)  that is used for plotting\n",
    "state_order_density_heatmap = pd.merge(geodf, state_oder_density, left_on = 'sigla', right_on = 'customer_state', how = 'inner')\n",
    "state_order_density_heatmap = state_order_density_heatmap[['customer_state', 'count', 'geometry']]\n",
    "state_order_density_heatmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_order_density_heatmap['coords'] = state_order_density_heatmap['geometry'].apply(lambda x: x.representative_point().coords[:]) # Get coordinates from geometry as a new column\n",
    "state_order_density_heatmap['coords'] = [coords[0] for coords in state_order_density_heatmap['coords']] # Unpack the list\n",
    "state_order_density_heatmap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, a sentiment proportion of each customer states were created to view the snapshot of sentiment proportions per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_proportion_customer = stateloc_final.groupby(['customer_state','class']).size().reset_index().rename(columns = {0 : 'count'})\n",
    "sentiment_proportion_customer['proportion'] = sentiment_proportion_customer['count'].div(sentiment_proportion_customer.groupby('customer_state')['count'].transform('sum'))\n",
    "sentiment_proportion_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.1.2. HOW'S THE OVERALL ORDER REVIEWS AT A GLANCE? IS THERE ANY DIFFERENCE IN REVIEW SCORE BETWEEN COMMENT AND NO COMMENT ORDERS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this visualization, we would use the raw reviews (without missing values removed) to get the fuller picture of the data. Hence, we imported again the raw version of `reviews` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_csv('Dataset/olist_order_reviews_dataset.csv')\n",
    "reviews_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data frame containing the count and proportion for each `review_score` is constructed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count = reviews_raw.groupby('review_score').count()['review_id'].reset_index()\n",
    "review_count['proportion'] = review_count['review_id'].apply(lambda x: x / review_count['review_id'].sum() * 100).round(2)\n",
    "review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested to check the proportion of orders which receive comment and do not. It could be seen in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_received = reviews_raw['review_comment_message'].isna().value_counts()\n",
    "comments_received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to construct a wordcloud plot that provides a snapshot of the most frequent terms in the reviews. For convenience, all the text were translated to english using the following code (Commented). We did it partially to avoid RTO error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preliminary info for this part\n",
    "# # Split the translate into 8 part. Export so we don't need to do it over and over. use the exported data\n",
    "# # Why we're doing it this way ? Because doing all at once takes too long. And in the process, if it takes too long, the process somehow suddenly stop because of certain error.\n",
    "\n",
    "# # Divided into these Parts\n",
    "# # reviews[\"review_comment_message\"][:5000] - Done\n",
    "# # reviews[\"review_comment_message\"][5000:10000] - Done\n",
    "# # reviews[\"review_comment_message\"][10000:15000] - Done\n",
    "# # reviews[\"review_comment_message\"][15000:20000] - Done\n",
    "# # reviews[\"review_comment_message\"][20000:25000] - otw\n",
    "# # reviews[\"review_comment_message\"][25000:30000] - Done\n",
    "# # reviews[\"review_comment_message\"][30000:35000] - otw\n",
    "\n",
    "# # Define the translator\n",
    "# translator = Translator()\n",
    "\n",
    "# # Make a list to compile translated document\n",
    "# translated_document_list = []\n",
    "\n",
    "# # Which document to be translated \n",
    "# package_document = reviews[\"review_comment_message\"][30000:]\n",
    "\n",
    "# # Using for loop to translate the document\n",
    "# for i, document in enumerate(package_document):\n",
    "#     # To know the progress, using this part\n",
    "#     if i % 500 == 0:\n",
    "#         print(i)\n",
    "        \n",
    "#     # The real translation process start here\n",
    "#     translation = translator.translate(document, dest = \"en\")\n",
    "#     translated_document = translation.text\n",
    "    \n",
    "#     # Append the translated document into empty list.\n",
    "#     translated_document_list.append(translated_document)\n",
    "# else : \n",
    "#     # We know when the translation process is finished.\n",
    "#     print(\"translation finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the translation are stored in `Resources/Translation` folder and as such it could be used directly. All the translated data frame are merged before any further manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the exported data into 1 list\n",
    "df_review_comment_1 = pd.read_excel(\"Resources/Translation/P_review_comment_en_0_5000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_2 = pd.read_excel(\"Resources/Translation/P_review_comment_en_5000_10000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_3 = pd.read_excel(\"Resources/Translation/P_review_comment_en_10000_15000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_4 = pd.read_excel(\"Resources/Translation/P_review_comment_en_15000_20000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_5 = pd.read_excel(\"Resources/Translation/P_review_comment_en_20000_25000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_6 = pd.read_excel(\"Resources/Translation/P_review_comment_en_25000_30000_36krows.xlsx\", usecols = [1])\n",
    "df_review_comment_7 = pd.read_excel(\"Resources/Translation/P_review_comment_en_30000_end_36krows.xlsx\", usecols = [1])\n",
    "\n",
    "# Combine all frames in 1 list\n",
    "df_to_combine = [df_review_comment_1, df_review_comment_2, df_review_comment_3, df_review_comment_4,\n",
    "                 df_review_comment_5, df_review_comment_6, df_review_comment_7]\n",
    "\n",
    "# Combine all dataframe\n",
    "df_review_comment_message_en = pd.concat(df_to_combine, axis = 0)\n",
    "df_review_comment_message_en.rename(columns = {0 : \"review_comment_message_en\"}, inplace = True)\n",
    "df_review_comment_message_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 5280, height = 720, background_color = 'white', max_words = 100).generate(str(df_review_comment_message_en['review_comment_message_en'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the distribution of `review_score` for orders with comment and without comment, we need to first create a new column that describes the comment class. Then, for each group, a data frame grouped by `review_score` would be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw['comment'] = np.where(reviews_raw['review_comment_message'].isna(), 'No comment', 'Comment')\n",
    "reviews_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_comment = reviews_raw[reviews_raw['comment'] == 'Comment'].groupby('review_score')['review_id'].count().reset_index()\n",
    "review_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_no_comment = reviews_raw[reviews_raw['comment'] == 'No comment'].groupby('review_score')['review_id'].count().reset_index()\n",
    "review_no_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check the hypothesis that orders without comment have a higher average `review_score` than orders with comment. To further support the hypothesis, the sample averages for orders with comment and without comment group are provided in the following pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_comment = reviews_raw.pivot_table(index = ['comment'], values = ['review_score'], aggfunc = {'review_score': ['mean', 'median']})\n",
    "pivot_comment.columns = pivot_comment.columns.droplevel()\n",
    "pivot_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.1.3. HOW'S THE DIFFERENCE OF DELIVERY PATH BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the delivery path for positive and negative sentiments group, we simply perform conditional selection on `stateloc_final` based on `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_final_0 = stateloc_final[stateloc_final['class'] == 0]\n",
    "stateloc_final_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateloc_final_1 = stateloc_final[stateloc_final['class'] == 1]\n",
    "stateloc_final_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.1.4. HOW'S THE DISTRIBUTION OF DELIVERY DATES BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS? IS IT STATISTICALLY DIFFERENT?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would rearrange `table2` by `seller_id` and perform some manipulations on the datetime columns to find the difference between estimated delivery date and actual delivery date.\n",
    "\n",
    "This table is indexed by `seller_id` to know which `seller_state` that has more out of time deliveries than on time deliveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_delivery = table2[['seller_id', 'order_purchase_year', 'order_purchase_year_month','order_purchase_timestamp', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'seller_city', 'seller_state', 'review_score']]\n",
    "seller_delivery.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the missing values of `seller_delivery` is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_delivery.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below performs all the necessary data frame manipulations to obtain the difference between actual delivery time and estimated delivery time. In short, it accounts for:\n",
    "1. Dropping missing values\n",
    "1. Defining `actual_delivery_time`, `estimated_delivery_time`, and `day_difference`\n",
    "1. Converting the timedeltas into `str` for extraction\n",
    "1. Converting the timedeltas into `int` for final usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values\n",
    "seller_delivery.dropna(inplace = True)\n",
    "\n",
    "# Taking the difference of days between columns\n",
    "seller_delivery['actual_delivery_time'] = seller_delivery['order_delivered_customer_date'] - seller_delivery['order_purchase_timestamp'] # Time taken from order to actual delivery time\n",
    "seller_delivery['estimated_delivery_time'] = seller_delivery['order_estimated_delivery_date'] - seller_delivery['order_purchase_timestamp'] # Time taken from order to estimated delivery time\n",
    "seller_delivery['day_difference'] = seller_delivery['actual_delivery_time'] - seller_delivery['estimated_delivery_time']  # The difference between actual and estimated delivery time\n",
    "\n",
    "# Converting the timedeltas into str\n",
    "seller_delivery['actual_delivery_time'] = seller_delivery['actual_delivery_time'].astype(str)\n",
    "seller_delivery['estimated_delivery_time'] = seller_delivery['estimated_delivery_time'].astype(str)\n",
    "seller_delivery['day_difference'] = seller_delivery['day_difference'].astype(str)\n",
    "\n",
    "separator_actual = seller_delivery['actual_delivery_time'].str.rsplit(' ', 2, expand = True)\n",
    "separator_actual.columns = ['actual_time', '1', '2']\n",
    "separator_actual = separator_actual.drop(columns = ['1', '2'])\n",
    "seller_delivery = pd.concat([seller_delivery, separator_actual], axis = 1)\n",
    "\n",
    "separator_estimated = seller_delivery['estimated_delivery_time'].str.rsplit(' ', 2, expand = True)\n",
    "separator_estimated.columns = ['estimated_time', '1', '2']\n",
    "separator_estimated = separator_estimated.drop(columns = ['1', '2'])\n",
    "seller_delivery = pd.concat([seller_delivery, separator_estimated], axis = 1)\n",
    "\n",
    "separator_day_diff = seller_delivery['day_difference'].str.rsplit(' ', 2, expand = True)\n",
    "separator_day_diff.columns = ['difference_time', '1', '2']\n",
    "separator_day_diff = separator_day_diff.drop(columns = ['1', '2'])\n",
    "seller_delivery = pd.concat([seller_delivery, separator_day_diff], axis = 1)\n",
    "\n",
    "# Converting the timedeltas into int\n",
    "seller_delivery['actual_time'] = seller_delivery['actual_time'].astype(int)\n",
    "seller_delivery['estimated_time'] = seller_delivery['estimated_time'].astype(int)\n",
    "seller_delivery['difference_time'] = seller_delivery['difference_time'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would classify orders according to their `difference_time`:\n",
    "\n",
    "$$\\texttt{difference\\_time} = \\begin{cases}\n",
    "\\leq 0 \\Rightarrow \\text{On Time} \\\\\n",
    "> 0 \\Rightarrow \\text{Out of Time}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_delivery['delivery_status'] = np.where(seller_delivery['difference_time'] <= 0, 'On Time', 'Out of Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_delivery.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we would see the distribution of `delivery_status` in total and on year basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_status = seller_delivery.groupby('delivery_status').count()['seller_id'].reset_index()\n",
    "delivery_status['proportion'] = delivery_status['seller_id'].apply(lambda x: x / sum(delivery_status['seller_id']) * 100).round(2)\n",
    "delivery_status.rename(columns = {'seller_id' : 'count'}, inplace = True)\n",
    "delivery_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_status_year = seller_delivery.groupby(['order_purchase_year', 'delivery_status']).count()['seller_id'].reset_index()\n",
    "delivery_status_year['proportion'] = delivery_status_year['seller_id'].div(delivery_status_year.groupby('order_purchase_year')['seller_id'].transform('sum')).round(2)\n",
    "delivery_status_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the distribution of `review_score` for on time and out of time orders, we need to first create a new column that describes the delivery status class. Then, for each group, a data frame grouped by `review_score` would be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_on_time = seller_delivery[seller_delivery['delivery_status'] == 'On Time'].groupby('review_score')['seller_id'].count().reset_index()\n",
    "review_on_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_out_of_time = seller_delivery[seller_delivery['delivery_status'] == 'Out of Time'].groupby('review_score')['seller_id'].count().reset_index()\n",
    "review_out_of_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check the hypothesis that on time orders have a higher average `review_score` than out of time orders. To further support the hypothesis, the sample averages for on time and out of time order groups are provided in the following pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_delivery_status = seller_delivery.pivot_table(index = ['delivery_status'], values = ['review_score'], aggfunc = {'review_score': ['mean', 'median']})\n",
    "pivot_delivery_status.columns = pivot_delivery_status.columns.droplevel()\n",
    "pivot_delivery_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-1-5\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.1.5. HOW'S SELLER STATE'S PERFORMANCE IN TERMS OF DELIVERY DAY DIFFERENCE?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to see the performance of each seller state in terms of the difference between actual and estimated delivery time (`difference_time`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_diff = seller_delivery.groupby('seller_state').mean()['difference_time'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_diff.sort_values('difference_time', ascending = True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_diff.sort_values('difference_time', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 2 tables above, we could see that even for the worst performing seller_state, the order delivery performance on average is still on time. Hence, overall, Olist has done a **good job** of maintaining the delivery performance accross all `seller_state`.\n",
    "\n",
    "Now, we want to divide the data in terms of `delivery_status` to see should the delivery is either on time or out of time, how would the extremes look like for each `seller_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_on_time = seller_delivery[seller_delivery['delivery_status'] == 'On Time'].groupby('seller_state')[['difference_time', 'seller_id']].agg(\n",
    "                                                                                                                                                    {\n",
    "                                                                                                                                                    'difference_time' : [('mean_difference_time', 'mean')],\n",
    "                                                                                                                                                    'seller_id' : [('total_count', 'count')]\n",
    "                                                                                                                                                    }\n",
    "                                                                                                                                               )\n",
    "states_on_time.columns = states_on_time.columns.droplevel()\n",
    "states_on_time = states_on_time.sort_values('mean_difference_time', ascending = True).head(7)\n",
    "states_on_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_out_of_time = seller_delivery[seller_delivery['delivery_status'] == 'Out of Time'].groupby('seller_state')[['difference_time', 'seller_id']].agg(\n",
    "                                                                                                                                                        {\n",
    "                                                                                                                                                        'difference_time' : [('mean_difference_time', 'mean')],\n",
    "                                                                                                                                                        'seller_id' : [('total_count', 'count')]\n",
    "                                                                                                                                                        }\n",
    "                                                                                                                                                   )\n",
    "states_out_of_time.columns = states_out_of_time.columns.droplevel()\n",
    "states_out_of_time = states_out_of_time.sort_values('mean_difference_time', ascending = False).head(7)\n",
    "states_out_of_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>3.2. DATA VISUALIZATION AND STATISTICS</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.2.1. HOW'S THE ORDER DENSITY ACROSS STATES? HOW'S THEIR SENTIMENT PROPORTIONS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_oder_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(('seaborn', 'ggplot')):\n",
    "    state_order_density_heatmap.plot(column = 'count', cmap = 'RdPu', legend = True, edgecolor = 'black', figsize = (20, 15))\n",
    "    for idx, row in state_order_density_heatmap.iterrows(): # State labels\n",
    "        plt.annotate(text = row['customer_state'], xy = row['coords'], horizontalalignment = 'center', color = 'black', fontsize = 14)\n",
    "    plt.title('Order Density Heatmap Accross State', fontsize = 24, weight = 'bold', pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure aboves describe the distribution of order density of Olist across all states in Brazil. The more intense the color, the more orders are being processed. The southern part of Brazil has more order density then all the other parts, with the most being San Paulo (`SP`, $40.48 \\%$), followed by Rio de Janeiro (`RJ`, $13.97%$), and Minas Gerais (`MG`, $11.56\\%$), etc. The northern part of Brazil only constitutes for about $7.71\\%$, which is relatively low compared to the whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10)) # ax cannot be passed unto catplot\n",
    "g = sns.catplot(data = sentiment_proportion_customer, x = 'customer_state', y = 'proportion', hue = 'class', kind = 'bar', height = 10, aspect = 3, palette = sns.color_palette(['lightskyblue', 'lightcoral']))\n",
    "\n",
    "# Labels\n",
    "ax = g.facet_axis(0,0)\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + 0.015, \n",
    "            p.get_height() * 1.02, \n",
    "            '{0:.2f}'.format(p.get_height()), \n",
    "            color = 'black', rotation = 'horizontal', size = 'large')\n",
    "\n",
    "# Title\n",
    "plt.title('Sentiment Class Proportion for Every Customer States', fontsize = 24, weight = 'bold', pad = 20)\n",
    "\n",
    "# Axis\n",
    "plt.xlabel('States', fontsize = 16, labelpad = 20)\n",
    "plt.ylabel('Proportion', fontsize = 16, labelpad = 20)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all customer states have a good proportion of positive sentiments $(>60\\%)$ except for `AL`, `MA`, `RJ`, and `RR`. In this regard, Olist has done a remarkable performing their business such that the majority of customer state are classified to have positive sentiment class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> -  The southern part of Brazil has more order density then all the other parts, with the most being San Paulo (`SP`, $40.48 \\%$), followed by Rio de Janeiro (`RJ`, $13.97%$), and Minas Gerais (`MG`, $11.56\\%$), etc. The northern part of Brazil only constitutes for about $7.71\\%$, which is relatively low compared to the whole\n",
    "> - Almost all customer states have a good proportion of positive sentiments $(>60\\%)$ except for `AL`, `MA`, `RJ`, and `RR`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.2.2. HOW'S THE OVERALL ORDER REVIEWS AT A GLANCE? IS THERE ANY DIFFERENCE IN REVIEW SCORE BETWEEN COMMENT AND NO COMMENT ORDERS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (30, 15))\n",
    "\n",
    "# Axis definition\n",
    "gs = GridSpec(2, 3, figure = fig) # Use GridSpec when plotting multiple subplots in a more unrestricted manner\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "# Plot 1\n",
    "sns.barplot(data = review_count, x = 'review_score', y = 'review_id', color = 'dodgerblue', ax = ax1)\n",
    "\n",
    "# Axis\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.set_xlabel('Review score', size = 12)\n",
    "ax1.tick_params(labelsize = 14)\n",
    "ax1.set_ylabel('')\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Annotation\n",
    "for p, t in zip(ax1.patches, review_count['proportion'].tolist()):\n",
    "        ax1.annotate(f'{p.get_height():.0f} ({t}%)', (p.get_x() + 0.4, p.get_height()),\n",
    "                    ha = 'center', va = 'bottom', xytext = (0, 1), textcoords = 'offset points', color = 'black', size = 14)\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Review Score Count', size = 20, weight = 'bold', pad = 20)\n",
    "\n",
    "# Plot 2\n",
    "explode = [0.1, 0.01]\n",
    "ax2.pie(comments_received, autopct = '%.2f%%', shadow = True, pctdistance = 0.5, textprops = {'fontsize' : 14, 'color' : 'white'}, startangle = 45, explode = explode, colors = ['skyblue', 'lightgreen'])\n",
    "ax2.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "\n",
    "# Legend\n",
    "ax2.legend(labels = ['With comment', 'Without comment'], loc = 0, bbox_to_anchor = (1.01, 1))\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "\n",
    "# Title\n",
    "ax2.set_title('Comments Received Proportions', size = 20, weight = 'bold', pad = 20)\n",
    "\n",
    "# Plot 3\n",
    "sns.barplot(data = review_comment, x = 'review_score', y = 'review_id', color = 'skyblue', alpha = 0.8, label = 'With comment',ax = ax3) \n",
    "sns.barplot(data = review_no_comment, x = 'review_score', y = 'review_id', color = 'lightgreen', alpha = 0.6, label = 'Without comment', ax = ax3)\n",
    "\n",
    "# Title\n",
    "ax3.set_title('Review Score Distribution by Comment', size = 20, weight = 'bold')\n",
    "\n",
    "# Axis\n",
    "ax3.set_xlabel('Review score', fontsize = 18, labelpad = 12)\n",
    "ax3.set_ylabel('Count', fontsize = 18, labelpad = 12)\n",
    "ax3.tick_params(labelsize = 14)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "\n",
    "# Legend\n",
    "# For manually stacked plot, legend is obtained with the 'label' argument\n",
    "\n",
    "# Plot 4\n",
    "ax4.imshow(wordcloud)\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Most Commented Words', size = 20, weight = 'bold', pad = 20)\n",
    "\n",
    "plt.suptitle('Reviews Overview', size = 28, weight = 'bold', y = 1.04)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Review Score Count**: <br> Reviews are dominated with the score 5 $(57.78\\%)$, followed by 4 ($18.29\\%$), and 1 ($11.51\\%$), while 3 and 2 only constitutes for less than $12\\%$ proportion. This data suggests that the customers tend to skip the in between score (2 and 3) right to the extremes if the order performance is poor. Overall, the performance bias is very good\n",
    "- **Comments Received Proportions**: <br> Over $58.70\\%$ orders received comments while the rest do not $(41.30\\%)$\n",
    "- **Review Score Distribution by Comment**: <br> Now we're interested to see if there is a difference in review score distribution between orders with comment and orders without comment group. At a glance, it seems like orders without comment have a larger proportion of high review scores (4, 5) and smaller proportion of low review scores (1, 2). But to test this hypothesis, we would conduct a statistical test in the following section\n",
    "- **Most Commented Words**: <br> The snapshot of most commented words revolves around the terms product, delivery, time, recommend. It seems like customers mostly comment about the product itself and/or the delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors conducted statistical test to know if the average `review_score` of is different between comment and no comment orders. But first, we need to examine the normality of `review_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat, pval = shapiro(reviews_raw['review_score'])\n",
    "print(shapiro(reviews_raw['review_score']))\n",
    "\n",
    "# Conclusion\n",
    "if pval < 0.05 :\n",
    "    print('Reject H0, not normal')\n",
    "else :\n",
    "    print('Accept H0, normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The distribution of `review_score` is NOT normal. Hence we use Mann-Whitney U-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mann-Whitney U-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis** <br> <br>\n",
    "$H_0:$ $\\text{median\\_review\\_score}_\\text{comment} = \\text{median\\_review\\_score}_\\text{no comment}$ <br>\n",
    "$H_1:$ $\\text{median\\_review\\_score}_\\text{comment} \\neq \\text{median\\_review\\_score}_\\text{no comment}$ <br>\n",
    "$\\alpha = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat, pval = mannwhitneyu(\n",
    "\t\t\t\t\t\t   reviews_raw[reviews_raw['comment'] == 'Comment']['review_score'],\n",
    "                           reviews_raw[reviews_raw['comment'] == 'No comment']['review_score'],\n",
    "                           alternative = 'two-sided')\n",
    "print(f'Test statistic = {tstat}, p-value = {pval}')\n",
    "\n",
    "# Conclusion\n",
    "if pval < 0.05 :\n",
    "    print('Reject H0')\n",
    "else: \n",
    "    print('Accept H0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is inferred that (at $\\alpha = 5\\%$) the median review score of orders with comment ordered is different than median review score of orders without comment. The slight difference might is not caused due to chance alone. Hence, customers that do not comment might have a give statistically different review score than those who do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pivot_comment.plot(kind = 'bar', color = ['dodgerblue', 'skyblue'])\n",
    "fig = ax.get_figure()\n",
    "fig.set_size_inches(14, 6)\n",
    "\n",
    "# Axis\n",
    "ax.set_xlabel('Comment status', fontsize = 14, labelpad = 10)\n",
    "ax.set_ylabel('Averages', fontsize = 14, labelpad = 10)\n",
    "ax.tick_params(labelsize = 12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Legend\n",
    "ax.legend(labels = ['Mean', 'Median'], loc = 1, bbox_to_anchor = (1.01, 1))   \n",
    "\n",
    "# Annotation    \n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:,.2f}'.format(p.get_height()), (p.get_x() + 0.12, p.get_height()),\n",
    "                    ha = 'center', va = 'bottom', xytext = (0, 1), textcoords = 'offset points',\n",
    "                    color = 'black', fontsize = 12)\n",
    "\n",
    "ax.set_title('Comment Status Averages', size = 16, weight = 'bold', pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the averages of review score based on comment status. Keep in mind that this does not represent the whole population as this is only a sample. However, from this, we could see that the mean review score of no comment group $(3.67, 5.00)$ is larger than the comment group $(4.38, 5.00)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - Overall, most orders have a review score of 5 $(57.78\\%)$, followed by 4 $(19.29\\%)$ and closely by 1 $(11.51\\%)$. Meanwhile, for review score of 2 and 3 constitute no more than $12\\%$. This data suggests that the customers tend to skip the in between score (2 and 3) right to the extremes if the order performance is poor\n",
    "> - Over $58.70\\%$ orders received comments while the rest do not $(41.30\\%)$\n",
    "> - At a glance, it seems like orders without comment have a larger proportion of high review scores (4, 5) and smaller proportion of low review scores (1, 2)\n",
    "> - After statistical test, it is inferred that (at $\\alpha = 5\\%$) the median review score of orders with comment ordered is different than median review score of orders without comment. Hence, customers that do not comment might have a give statistically different review score than those who do\n",
    "> - The snapshot of most commented words revolves around the terms product, delivery, time, recommend. It seems like customers mostly comment about the product itself and/or the delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.2.3. HOW'S THE DIFFERENCE OF DELIVERY PATH BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# with plt.style.context(('seaborn', 'ggplot')):\n",
    "#     world[world.name == 'Brazil'].plot(figsize = (15, 15), edgecolor = 'grey', color = 'white')\n",
    "#     for clat, slat, clng, slng in zip(stateloc_final_0['customer_geolocation_lat'], stateloc_final_0['seller_geolocation_lat'], stateloc_final_0['customer_geolocation_lng'], stateloc_final_0['seller_geolocation_lng']):\n",
    "#         plt.plot([clng, slng], [clat, slat], color = 'lightskyblue', alpha = 0.01)\n",
    "#         plt.scatter([clng, slng], [clat, slat], color = 'grey', alpha = 0.01)\n",
    "#     plt.title('Connection Map of Orders with Positive Sentiments', fontsize = 24, weight = 'bold', pad = 20)\n",
    "#     plt.savefig(fname = 'positivemap.png', dpi = 300, format = 'png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "basewidth = 1200\n",
    "img = Image.open('Images/positivemap.png')\n",
    "wpercent = (basewidth / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "# with plt.style.context(('seaborn', 'ggplot')):\n",
    "#     world[world.name == 'Brazil'].plot(figsize = (15, 15), edgecolor = 'grey', color = 'white')\n",
    "#     for clat, slat, clng, slng in zip(stateloc_final_1['customer_geolocation_lat'], stateloc_final_1['seller_geolocation_lat'], stateloc_final_1['customer_geolocation_lng'], stateloc_final_1['seller_geolocation_lng']):\n",
    "#         plt.plot([clng, slng], [clat, slat], color = 'lightcoral', alpha = 0.01)\n",
    "#         plt.scatter([clng, slng], [clat, slat], color = 'grey', alpha = 0.01)\n",
    "#     plt.title('Connection Map of Orders with Negative Sentiments', fontsize = 24, weight = 'bold', pad = 20)\n",
    "#     plt.savefig(fname = 'negativemap.png', dpi = 300, format = 'png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "basewidth = 1200\n",
    "img = Image.open('Images/negativemap.png')\n",
    "wpercent = (basewidth / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> From these 2 charts, there seems to be no difference in customer sentiments in terms of delivery path (i.e. distance). This suggests that in the end, distance may not play part in the overall customer sentiment, as long as the delivery process is under control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.2.4. HOW'S THE DISTRIBUTION OF DELIVERY DATES BETWEEN ORDERS WITH POSITIVE AND NEGATIVE SENTIMENTS? IS IT STATISTICALLY DIFFERENT?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (30, 20))\n",
    "\n",
    "# Axis definition\n",
    "gs = GridSpec(2, 2, figure = fig) # Use GridSpec when plotting multiple subplots in a more unrestricted manner\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Plot 1\n",
    "explode = [0.1, 0.01]\n",
    "ax1.pie(delivery_status['count'], shadow = True, autopct = '%.2f%%', pctdistance = 0.5, textprops = {'fontsize' : 18, 'color' : 'white'}, startangle = 45, explode = explode, colors = ['yellowgreen', 'coral'])\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "\n",
    "# Legend\n",
    "ax1.legend(labels = ['On Time', 'Out of Time'], loc = 0, bbox_to_anchor = (1.01, 1))\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Delivery Status Proportions', size = 20, weight = 'bold')\n",
    "\n",
    "# Plot 2\n",
    "sns.barplot(data = delivery_status_year, x = 'order_purchase_year', y = 'proportion', hue = 'delivery_status', ax = ax2, palette = ['yellowgreen', 'coral'])\n",
    "\n",
    "# Annotation\n",
    "for p in ax2.patches:\n",
    "        ax2.annotate(f'{p.get_height():.2%}', (p.get_x() + 0.2, p.get_height()),\n",
    "                    ha = 'center', va = 'bottom', xytext = (0, 1), textcoords = 'offset points',\n",
    "                    color = 'black', fontsize = 16)\n",
    "\n",
    "# Legend\n",
    "plt.rcParams.update({'legend.fontsize': 12})\n",
    "\n",
    "# Axis\n",
    "ax2.set_xlabel('Year', fontsize = 18, labelpad = 10)\n",
    "ax2.set_ylabel('Proportion', fontsize = 18, labelpad = 10)\n",
    "ax2.tick_params(labelsize = 14)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "# Title\n",
    "ax2.set_title('Delivery Status per Year', size = 20, weight = 'bold')\n",
    "\n",
    "# Plot 3\n",
    "sns.histplot(data = seller_delivery, x = 'difference_time', hue = 'delivery_status', kde = True, palette = ['yellowgreen', 'coral'], ax = ax3)\n",
    "\n",
    "# Limits\n",
    "ax3.set_xlim(-40, 20)\n",
    "\n",
    "# Vertical line\n",
    "ax3.vlines(x = seller_delivery['difference_time'].median(), ymin = 0, ymax = 2700, colors = 'grey', linewidths = 3)\n",
    "\n",
    "# Text\n",
    "ax3.annotate(f'Median day difference \\n is {seller_delivery[\"difference_time\"].median():.2f} days', xy = (seller_delivery[\"difference_time\"].median() + 0.5, 2500), \n",
    "             fontsize = 16, xytext = (seller_delivery[\"difference_time\"].median() + 4, 2510), va = 'center_baseline',\n",
    "             arrowprops = dict(facecolor = \"grey\"),\n",
    "             color = \"grey\", weight = \"bold\")\n",
    "\n",
    "# Title\n",
    "ax3.set_title('Day Difference Distribution', size = 20, weight = 'bold')\n",
    "\n",
    "# Axis\n",
    "ax3.set_xlabel('Day difference', fontsize = 18, labelpad = 12)\n",
    "ax3.set_ylabel('Count', fontsize = 18, labelpad = 12)\n",
    "ax3.tick_params(labelsize = 14)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "\n",
    "# Plot 4\n",
    "sns.barplot(data = review_on_time, x = 'review_score', y = 'seller_id', color = 'yellowgreen', ax = ax4)\n",
    "sns.barplot(data = review_out_of_time, x = 'review_score', y = 'seller_id', color = 'coral', ax = ax4)\n",
    "\n",
    "# Title\n",
    "ax4.set_title('Review Score Distribution by Delivery Status', size = 20, weight = 'bold')\n",
    "\n",
    "# Axis\n",
    "ax4.set_xlabel('Review score', fontsize = 18, labelpad = 12)\n",
    "ax4.set_ylabel('Count', fontsize = 18, labelpad = 12)\n",
    "ax4.tick_params(labelsize = 14)\n",
    "ax4.spines['top'].set_visible(False)\n",
    "ax4.spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Delivery Status', size = 28, weight = 'bold', y = 1.04)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Delivery Status Proportions**: <br> Over $90.30\\%$ orders are delivered on time while a small fraction of orders are delivered out of time $(9.70\\%)$\n",
    "- **Delivery Status per Year**: <br> The performance of delivery status per year underwent a slight decrease from $92\\%$ in 2017 went to $89\\%$ in 2018\n",
    "- **Day Difference Distribution**: <br> Overall, the day difference distribution is good, in which the median score for day difference is at $-12$ days. Olist have shown a good performance for order delivery timeliness\n",
    "- **Review Score Distribution by Delivery Status**: <br> Now we're interested to see if there is a difference in review score distribution between on time and out of time deliveries. At a glance, it seems like on time deliveries have a larger proportion of high review scores (4, 5) and smaller proportion of low review scores (1, 2). Meanwhile, out of time deliveries have a large proportion of low review score (1) only. But to test this hypothesis, we would conduct a statistical test in the following section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors conducted statistical test to know if the average `review_score` of is different between on time and out of time orders. But first, we need to examine the normality of `review_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat, pval = shapiro(seller_delivery['review_score'])\n",
    "print(shapiro(seller_delivery['review_score']))\n",
    "\n",
    "# Conclusion\n",
    "if pval < 0.05 :\n",
    "    print('Reject H0, not normal')\n",
    "else :\n",
    "    print('Accept H0, normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The distribution of `review_score` is NOT normal. Hence we use Mann-Whitney U-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mann-Whitney U-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis** <br> <br>\n",
    "$H_0:$ $\\text{median\\_review\\_score}_\\text{on time} = \\text{median\\_review\\_score}_\\text{out of time}$ <br>\n",
    "$H_1:$ $\\text{median\\_review\\_score}_\\text{on time} > \\text{median\\_review\\_score}_\\text{out of time}$ <br>\n",
    "$\\alpha = 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat, pval = mannwhitneyu(\n",
    "\t\t\t\t\t\t   seller_delivery[seller_delivery['delivery_status'] == 'On Time']['review_score'],\n",
    "                           seller_delivery[seller_delivery['delivery_status'] == 'Out of Time']['review_score'],\n",
    "                           alternative = 'greater')\n",
    "print(f'Test statistic = {tstat}, p-value = {pval}')\n",
    "\n",
    "# Conclusion\n",
    "if pval < 0.05 :\n",
    "    print('Reject H0')\n",
    "else: \n",
    "    print('Accept H0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is inferred that (at $\\alpha = 5\\%$) the median review score of on time deliveries is larger than median review score of out of time deliveries. The slight difference might is not caused due to chance alone. Hence, so far, delivery timeliness is the biggest factor for determining customer review score and overall satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pivot_delivery_status.plot(kind = 'bar', color = ['dodgerblue', 'skyblue'])\n",
    "fig = ax.get_figure()\n",
    "fig.set_size_inches(14, 6)\n",
    "\n",
    "ax.set_xlabel('Delivery status', fontsize = 14, labelpad = 10)\n",
    "ax.set_ylabel('Averages', fontsize = 14, labelpad = 10)\n",
    "ax.tick_params(labelsize = 12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Annotation    \n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:,.2f}'.format(p.get_height()), (p.get_x() + 0.12, p.get_height()),\n",
    "                    ha = 'center', va = 'bottom', xytext = (0, 1), textcoords = 'offset points',\n",
    "                    color = 'black', fontsize = 12)\n",
    "\n",
    "ax.set_title('Delivery Status Averages', size = 16, weight = 'bold', pad = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the averages of review score based on delivery status. Keep in mind that this does not represent the whole population as this is only a sample. However, from this, we could see that the mean and median review score of on time group $(3.97, 5.00)$ is larger than the comment group $(1.92, 1.00)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - $90.30 \\%$ of orders is on time while only $9.7 \\%$ is out of time. This is quite satisfactory for Olist\n",
    "> - The delivery status performance year over year is underwent a slight decrease. This means that Olist should maintain its delivery quality over the years\n",
    "> - Overall, the delivery timeliness performance of Olist is good, with items delivered $12$ days **before** the estimated time\n",
    "> - At a glance, it seems like on time deliveries have a larger proportion of high review scores (4, 5) and smaller proportion of low review scores (1, 2). Meanwhile, out of time deliveries have a large proportion of low review score (1) only\n",
    "> - After statistical test, It is inferred that (at $\\alpha = 5\\%$) the median review score of on time deliveries is larger than median review score of out of time deliveries. Hence, so far, delivery timeliness is the biggest factor for determining customer review score and overall satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-2-5\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>3.2.5. HOW'S SELLER STATE'S PERFORMANCE IN TERMS OF DELIVERY DAY DIFFERENCE?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-press\n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout = True, figsize = (24, 6))\n",
    "\n",
    "# Axis definition\n",
    "gs = GridSpec(1, 2, figure = fig) # Use GridSpec when plotting multiple subplots in a more unrestricted manner\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "# Plot 1\n",
    "sns.barplot(data = states_out_of_time, y = states_out_of_time.index, x = states_out_of_time['mean_difference_time'], color = 'darkblue', ax = ax1)\n",
    "\n",
    "# Annotation\n",
    "for rect in ax1.patches:\n",
    "    ax1.annotate('{:,.0f} days'.format(rect.get_width()),(rect.get_width(),rect.get_y() + rect.get_height() / 2),\n",
    "                xytext = (0, 0),textcoords = 'offset points', va = 'center', ha = 'right', size = 12, color = 'white')\n",
    "\n",
    "# Axes\n",
    "ax1.set_ylabel('')\n",
    "ax1.tick_params(labelsize = 12)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['left'].set_visible(False)\n",
    "ax1.spines['bottom'].set_visible(False)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "\n",
    "# Title\n",
    "ax1.set_title('Slowest Average Delivery Time (Out of Time) by Seller States', size = 16, weight = 'bold')\n",
    "\n",
    "# Plot 2\n",
    "sns.barplot(data = states_on_time, y = states_on_time.index, x = states_on_time['mean_difference_time'], color = 'dodgerblue', ax = ax2)\n",
    "\n",
    "# Annotation\n",
    "for rect in ax2.patches:\n",
    "    ax2.annotate('{:,.0f} days'.format(rect.get_width()),(rect.get_width(),rect.get_y() + rect.get_height() / 2),\n",
    "                xytext = (0, 0),textcoords = 'offset points', va = 'center', ha = 'left', size = 12, color = 'white')\n",
    "\n",
    "# Axes\n",
    "ax2.set_ylabel('')\n",
    "ax2.tick_params(labelsize = 12)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['left'].set_visible(False)\n",
    "ax2.spines['bottom'].set_visible(False)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "\n",
    "# Title\n",
    "ax2.set_title('Fastest Average Delivery Time (On Time) by Seller States', size = 16, weight = 'bold')\n",
    "\n",
    "plt.suptitle('Seller States Delivery Day Difference', size = 24, weight = 'bold', y = 1.04)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the performance of individual seller states in both out of time and on time deliveries. The most extreme states are `CE` at the average of $30$ days late, and `RO` at $-24$ days earlier. Keep in mind that this chart is produced by splitting the data in terms of `delivery_status` to see should the delivery is either on time or out of time, how would the extremes look like for each `seller_state`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - The most extreme states are `CE` at the average of $30$ days late, and `RO` at $-24$ days earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>3.3. TABLEAU DASHBOARD</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1656519073709' style='position: relative'><noscript><a href='#'><img alt='1_DB ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Br&#47;BrazillianEcommerceKaggle&#47;1_DB&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='BrazillianEcommerceKaggle&#47;1_DB' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Br&#47;BrazillianEcommerceKaggle&#47;1_DB&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1656519073709');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1700px';vizElement.style.height='877px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1700px';vizElement.style.height='877px';} else { vizElement.style.width='100%';vizElement.style.height='1477px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1656518781100' style='position: relative'><noscript><a href='#'><img alt='2_DB ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Br&#47;BrazillianEcommerceKaggle&#47;2_DB&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='BrazillianEcommerceKaggle&#47;2_DB' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Br&#47;BrazillianEcommerceKaggle&#47;2_DB&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1656518781100');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1700px';vizElement.style.height='927px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1700px';vizElement.style.height='927px';} else { vizElement.style.width='100%';vizElement.style.height='1477px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>4. MACHINE LEARNING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.1. DATA PREPARATION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import orders & reviews again\n",
    "orders = pd.read_csv('Dataset/olist_orders_dataset.csv')\n",
    "reviews = pd.read_csv('Dataset/olist_order_reviews_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGING TABLE DATA (MACHINE LEARNING)**<br><br>\n",
    "\n",
    "Start by merging our data (`orders` & `reviews`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for machine learning\n",
    "df_0_model = orders.copy(deep = True)\n",
    "print('dataframe dimension = ',df_0_model.shape)\n",
    "df_0_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data with reviews data\n",
    "df_0_model = pd.merge(df_0_model, reviews, how = 'left', left_on = 'order_id', right_on = 'order_id')\n",
    "print('dataframe dimension = ',df_0_model.shape)\n",
    "df_0_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We understand that from 1 `order_id` (`8e17072ec97ce29f0e1f111e598b0c85`) can contain 3 different `review_id` containing different review comment message as well\n",
    "\n",
    "* If we see from `review_id`, we understand that in 1 `review_id` 1 `review_id` (`dbdf1ea31790c8ecfcc6750525661a9b`) can contain 3 different order_id but same review comment message\n",
    "\n",
    "* Remember that we're gonna do sentiment analysis, so we're gonna need to get as much review comment message as we can from the data. with that in mind, let's join our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, We're gonna show only the data that will be used in the next process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model = df_0_model[['review_id', 'review_comment_message', 'review_score']]\n",
    "print('dataframe dimension = ',df_0_model.shape)\n",
    "df_0_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing any further, We gonna check for duplicated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated data\n",
    "df_0_model.duplicated().value_counts()\n",
    "\n",
    "# Based on the output below, we know that we have duplicated value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `review_id` is not giving us any information, we can drop that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output above we know that we have duplicate value. We drop the duplicate value in this section below. We will drop about 1581 rows of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to remove duplicates\n",
    "df_0_model.drop_duplicates(inplace = True)\n",
    "\n",
    "# Make sure that we don't have dupicate data anymore\n",
    "df_0_model.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to check our data, In this section we gonna check for missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing value\n",
    "(df_0_model.isna().sum()*100/df_0_model.shape[0]).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got $58.68\\%$ missing value for review_comment_message (from 98411 rows of data). Since we can't do anything to fill the `review_comment_message`, we'll drop the missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing value\n",
    "df_0_model.dropna(inplace = True)\n",
    "\n",
    "# Check for missing value again\n",
    "(df_0_model.isna().sum()*100/df_0_model.shape[0]).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our data shape\n",
    "df_0_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we're gonna drop `review_id` because it's not giving us any information. By removing `review_id` we can check for duplicate value in our content (`review_comment_message` & `review_score`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop review_id\n",
    "df_0_model.drop('review_id', axis = 1, inplace = True)\n",
    "\n",
    "# Check data shape\n",
    "print(df_0_model.shape)\n",
    "\n",
    "# Check the outpout\n",
    "df_0_model.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate value in review_comment_message & review_score\n",
    "df_0_model.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to remove duplicates\n",
    "df_0_model.drop_duplicates(inplace = True)\n",
    "\n",
    "# Make sure that we don't have dupicate data anymore\n",
    "df_0_model.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after we drop significant amount of data because of missing value, we still have 36804 rows of data to work with. We can still do Sentiment Analysis with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do sentiment analysis, we need to know which review_comment_message giving `PositiveSentiment` or `NegativeSentiment`. We determine that :<br>\n",
    "\n",
    "$$\\text{Rating} \\in [1,5] \\,\\, \\begin{cases} \\leq 3 \\Rightarrow \\text{Negative sentiment} \\\\ > 3 \\Rightarrow \\text{Positive sentiment} \\end{cases}$$\n",
    "\n",
    "We make new column named `label` and apply those rules. Then we can drop `review_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new column : label and apply the rules\n",
    "df_0_model['label'] = df_0_model['review_score'].apply(lambda x : 'PositiveSentiment' if x > 3 else 'NegativeSentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Check the target distribution first before we continue\n",
    "col_info(df_0_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the output above, we know that we have $65\\%$ of `PositiveSentiment` and $35\\%$ `NegativeSentiment` which make our data imbalanced\n",
    "* To improve Olist, we can learn from `review_comment_message` from `NegativeSentiment`, `PositiveSentiment` gives us what quality to maintain. Hence we'll focused on NegativeSentiment\n",
    "* For Machine Learning we can't give label just as it is. We need to convert to something that machine can comprehend\n",
    "    * 1 for `NegativeSentiment`\n",
    "    * 0 for `PositiveSentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply those information to df_0_model[\"label\"]\n",
    "df_0_model['label'].replace('PositiveSentiment', 0, inplace = True)\n",
    "df_0_model['label'].replace('NegativeSentiment', 1, inplace = True)\n",
    "\n",
    "# Make sure that the change is made\n",
    "col_info(df_0_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need `review_message_comment` and label to do sentiment analysis, hence we drop the column that we're not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset_index \n",
    "df_0_model.reset_index(inplace = True)\n",
    "\n",
    "# Drop unnecessary column\n",
    "df_0_model.drop(columns= ['index', 'review_score'], inplace = True)\n",
    "\n",
    "# Just showin the data\n",
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `review_comment_message` is in Portugese, we want to translate the `review_comment_message` to english (We think we need to give audience something that we can understand, hence need to translate to english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_0_model['review_comment_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preliminary info for this part\n",
    "# # Split the translate into 8 part. Export so we don't need to do it over and over. use the exported data\n",
    "# # Why we're doing it this way ? Because doing all at once takes too long. And in the process, if it takes too long, the process somehow suddenly stop because of certain error.\n",
    "\n",
    "# # Divided into these Parts\n",
    "# # df_0_model[\"review_comment_message\"][:5000] - Done\n",
    "# # df_0_model[\"review_comment_message\"][5000:10000] - Done\n",
    "# # df_0_model[\"review_comment_message\"][10000:15000] - Done\n",
    "# # df_0_model[\"review_comment_message\"][15000:20000] - Done\n",
    "# # df_0_model[\"review_comment_message\"][20000:25000] - otw\n",
    "# # df_0_model[\"review_comment_message\"][25000:30000] - Done\n",
    "# # df_0_model[\"review_comment_message\"][30000:35000] - otw\n",
    "\n",
    "# # define the translator\n",
    "# translator = Translator()\n",
    "\n",
    "# # Make a list to compile translated document\n",
    "# translated_document_list = []\n",
    "\n",
    "# # Which document to be translated \n",
    "# package_document = df_0_model[\"review_comment_message\"][30000:]\n",
    "\n",
    "# # Using for loop to translate the document\n",
    "# for i, document in enumerate(package_document):\n",
    "#     # To know the progress, using this part\n",
    "#     if i % 500 == 0:\n",
    "#         print(i)\n",
    "        \n",
    "#     # The real translation process start here\n",
    "#     translation = translator.translate(document, dest = \"en\")\n",
    "#     translated_document = translation.text\n",
    "    \n",
    "#     # Append the translated document into empty list.\n",
    "#     translated_document_list.append(translated_document)\n",
    "# else : \n",
    "#     # We know when the translation process is finished.\n",
    "#     print(\"translation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to translate 1000 document need around 1 minute & 50 second, let say 2 minute\n",
    "# # translate 40k document need 80 minute, around 1hour.\n",
    "# # Because there's so much time to do this\n",
    "\n",
    "# # made list into dataframe, then convert output to xlsx.\n",
    "# df_translated_review_comment = pd.DataFrame(translated_document_list)\n",
    "\n",
    "# # peek the result\n",
    "# df_translated_review_comment[:5]\n",
    "\n",
    "# # export to excel\n",
    "# df_translated_review_comment.to_excel(\"P_review_comment_en_30000_end_36krows.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the exported data into 1 list\n",
    "df_review_comment_1 = pd.read_excel('Resources\\Translation\\P_review_comment_en_0_5000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_2 = pd.read_excel('Resources\\Translation\\P_review_comment_en_5000_10000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_3 = pd.read_excel('Resources\\Translation\\P_review_comment_en_10000_15000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_4 = pd.read_excel('Resources\\Translation\\P_review_comment_en_15000_20000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_5 = pd.read_excel('Resources\\Translation\\P_review_comment_en_20000_25000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_6 = pd.read_excel('Resources\\Translation\\P_review_comment_en_25000_30000_36krows.xlsx', usecols = [1])\n",
    "df_review_comment_7 = pd.read_excel('Resources\\Translation\\P_review_comment_en_30000_end_36krows.xlsx', usecols = [1])\n",
    "\n",
    "# combine all frames in 1 list\n",
    "df_to_combine = [df_review_comment_1, df_review_comment_2, df_review_comment_3, df_review_comment_4,\n",
    "                 df_review_comment_5, df_review_comment_6, df_review_comment_7]\n",
    "\n",
    "# combine all dataframe\n",
    "df_review_comment_message_en = pd.concat(df_to_combine, axis = 0)\n",
    "df_review_comment_message_en.rename(columns = {0 : 'review_comment_message_en'}, inplace = True)\n",
    "df_review_comment_message_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if our data is align between original data and the translated one, we check based on index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Review Comment Message')\n",
    "print('==='*50)\n",
    "print(df_0_model['review_comment_message'].iloc[[2568, 7521, 12315, 22546, 27545, 32591]].values)\n",
    "print('\\n\\nReview Comment Message Translated')\n",
    "print('==='*50)\n",
    "print(df_review_comment_message_en['review_comment_message_en'].iloc[[2568, 7521, 12315, 22546, 27545, 32591]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on manual check above, we know that the data already align. So we can continue to the next step, Start by inserting the translated document to `df_0_model`. This is our base data to do sentiment analysis. either using translated review (In English) or original review (In Portugese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model.insert(1, 'review_comment_message_en', df_review_comment_message_en['review_comment_message_en'].values)\n",
    "df_0_model.head()\n",
    "\n",
    "# for precaution, I'll convert both review_comment_message & review_comment_message_en to string before continuing any further.\n",
    "df_0_model['review_comment_message'] = df_0_model['review_comment_message'].apply(lambda x : str(x))\n",
    "df_0_model['review_comment_message_en'] = df_0_model['review_comment_message_en'].apply(lambda x : str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since We want to analysis the `review_comment` in English, we can drop the original `review_comment_message` that come in Portugese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original review_comment_message\n",
    "df_0_model.drop('review_comment_message', axis = 1, inplace = True)\n",
    "\n",
    "# check our data\n",
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand that our document contain emoji's. We try to convert emoji into text in section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile list of emoji to a list\n",
    "# We're using set so that there's no duplicate value\n",
    "emoji_set_bulk = {'😊',}\n",
    "\n",
    "# Using forloop to only get the emoji from our corpus\n",
    "for doc in df_0_model['review_comment_message_en']:\n",
    "    # add the emoji to the empty Set we provide above\n",
    "    emoji_set_bulk.add(extract_emojis(doc))\n",
    "\n",
    "# Convert the set into list for easier processing\n",
    "emoji_list = list(emoji_set_bulk)\n",
    "\n",
    "# Delete first item on the list because it's not an emoji\n",
    "del emoji_list[0]\n",
    "\n",
    "# Check the output\n",
    "emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make an empty Set\n",
    "emoji_set_individual = {'😊',}\n",
    "\n",
    "# Using forloop to access the bulk emoji in emoji list.\n",
    "for bulk_emoji in emoji_list:\n",
    "    # Using defined function to split the emoji, and add it to new variables\n",
    "    splitted_emoji = split_emojis(bulk_emoji)\n",
    "    \n",
    "    # We understand from above process that the emoji has been splitted. then we can access per individual emoji in the list and add it to our set\n",
    "    for individual_emoji in splitted_emoji:\n",
    "        emoji_set_individual.add(individual_emoji)\n",
    "\n",
    "# Convert the set into a list for better processing\n",
    "emoji_list_individual = list(emoji_set_individual)\n",
    "\n",
    "# Get a meaning from our emojis\n",
    "for emot in emoji_list_individual:\n",
    "    print(emot, '  stands for ', emoji.UNICODE_EMOJI['en'][emot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with above information, we still lack the understanding of what the emojis means. Below I'll access each document that contain the emojis to understand them a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek based on the emoji [8599, 9128, 16520]\n",
    "df_0_model[df_0_model['review_comment_message_en'].str.contains('💖')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model['document_pp_emoji'] = df_0_model[['review_comment_message_en']].apply(replace_emoticon, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the converted emoji into text below and check the output until this steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model.loc[[8599, 9128, 16520]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - What we've done in this section \n",
    ">    - Preparing data for machine learning (sentiment analysis) by combining orders & reviews data\n",
    ">    - Data cleaning by removing missing values & duplicated data\n",
    ">    - Data cleaning by removing missing values & duplicated data\n",
    ">    - Label the data for machine learning based on review star\n",
    ">    - Translating the review comment message from Portugese to English\n",
    ">    - Understanding emoji and it's context in review comment message, then translate the emoji into text based on our understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.2. TEXT PREPROCESSING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we standardized the document with several steps as follows: \n",
    "* Transform the document to lowercase\n",
    "* Remove contractions\n",
    "* Remove numbers\n",
    "* Remove unnecessary whitespace\n",
    "* Remove stopwords\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column, for the document that has been preprocessed. first preprocessing to transform all document to lower case\n",
    "df_0_model['pp1_document'] = df_0_model['document_pp_emoji'].apply(to_lower)\n",
    "\n",
    "# Next preprocess is to remove contraction from the document\n",
    "df_0_model['pp1_document'] = df_0_model['pp1_document'].apply(main_contraction)\n",
    "\n",
    "# Next preprocess is to remove number\n",
    "df_0_model['pp1_document'] = df_0_model['pp1_document'].apply(remove_numbers)\n",
    "\n",
    "# Next preprocess is to remove punctuation\n",
    "df_0_model['pp1_document'] = df_0_model['pp1_document'].apply(remove_punct2)\n",
    "\n",
    "# Next preprocess is to remove unnecessary whitespace\n",
    "df_0_model['pp1_document'] = df_0_model['pp1_document'].apply(to_strip)\n",
    "\n",
    "# Next preprocess is to remove stopwords\n",
    "df_0_model['pp1_document'] = df_0_model['pp1_document'].apply(remove_stopwords)\n",
    "\n",
    "# Next preprocess is to lemmatize our document\n",
    "df_0_model['pp2_document_lemmatize'] = df_0_model['pp1_document'].apply(lemmatize)\n",
    "\n",
    "# Add new column for text_length_lemma\n",
    "df_0_model['text_length_lemma'] = df_0_model['pp2_document_lemmatize'].apply(len)\n",
    "\n",
    "# Check the output\n",
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much data we've got\n",
    "df_0_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model['text_length_lemma'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output above, we understand that processed text contain no characater at all (0 length character). In this steps we will drop 0 & 1 length character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model = df_0_model[df_0_model['text_length_lemma'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-3\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.3. SENTIMENT CONTENT & LENGTH OF SENTENCE</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going into Machine Learning, We want to know the content of `positive & negative sentiment`. We're gonna do that in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our dataframe\n",
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tf_tf_idf_explanation\"></a>\n",
    "\n",
    "**TF & TF - IDF**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$IDF = \\log\\left({\\frac{\\text{Total number of docs}}{\\text{Number of docs containing the words}}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we going into machine learning stuff, let's understand our text first. As we know that there's 2 `positive sentiment` & `negative sentiment`. let's see what's inside those sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document that will be used to check sentiment content\n",
    "positive_sentiment_text = df_0_model[df_0_model['label'] == 0]['pp2_document_lemmatize'] # This data is used to learn about the positive sentiment\n",
    "negative_sentiment_text = df_0_model[df_0_model['label'] == 1]['pp2_document_lemmatize'] # This data is used to learn about the negative sentiment\n",
    "\n",
    "# Extracting the top 10 onegram by sentiment\n",
    "onegram_pos = ngrams_count(positive_sentiment_text, (1, 1), 10)\n",
    "onegram_neg = ngrams_count(negative_sentiment_text, (1, 1), 10)\n",
    "\n",
    "# Extracting the top 10 bigrams by sentiment\n",
    "bigrams_pos = ngrams_count(positive_sentiment_text, (2, 2), 10)\n",
    "bigrams_neg = ngrams_count(negative_sentiment_text, (2, 2), 10)\n",
    "\n",
    "# Extracting the top 10 trigrams by sentiment\n",
    "trigrams_pos = ngrams_count(positive_sentiment_text, (3, 3), 10)\n",
    "trigrams_neg = ngrams_count(negative_sentiment_text, (3, 3), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary consist of all n-gram positive & negative sentiment to make the job easier\n",
    "ngram_dict_plot = {\n",
    "    'Top onegram on Positive Comments': onegram_pos,\n",
    "    'Top onegram on Negative Comments': onegram_neg,\n",
    "    'Top bigram on Positive Comments': bigrams_pos,\n",
    "    'Top bigram on Negative Comments': bigrams_neg,\n",
    "    'Top trigram on Positive Comments': trigrams_pos,\n",
    "    'Top trigram on Negative Comments': trigrams_neg,\n",
    "    }\n",
    "\n",
    "# Plot n - gram database\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize = (18, 15))\n",
    "i, j = 0, 0\n",
    "colors = ['Blues_r', 'Reds_r']\n",
    "for title, ngram_data in ngram_dict_plot.items():\n",
    "    ax = axs[i, j]\n",
    "    sns.barplot(x = 'count', y = 'ngram', data = ngram_data, ax = ax, palette = colors [j])\n",
    "    \n",
    "    # Customizing plots\n",
    "    ax.set_title(title, size=14)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.grid(visible = False)\n",
    "    ax.set_facecolor('xkcd:white')\n",
    "    \n",
    "    # Incrementing the index\n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        j = 0\n",
    "        i += 1\n",
    "                  \n",
    "fig.suptitle('Ngram count\\n', fontsize = 18, weight = 'bold', x = 0.55)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gonna make histogram for sentence length for positive sentiment & negative sentiment\n",
    "positive_sentiment = df_0_model[df_0_model['label'] == 0]\n",
    "negative_sentiment = df_0_model[df_0_model['label'] == 1]\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.distplot(positive_sentiment[\"text_length_lemma\"])\n",
    "sns.distplot(negative_sentiment[\"text_length_lemma\"])\n",
    "\n",
    "plt.title('Text Length Distribution', size = 16, weight = 'bold')\n",
    "\n",
    "plt.legend([\"positive_sentiment_length\", \"negative_sentiment_length\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> - For negative sentiments we can divide into 2 parts, negative sentiment with short & long review comment message\n",
    "> - For positive sentiments the review comment message tend to be short because they already satisfied with the product and maybe there's not much to talk about because the product is already great\n",
    "> - For negative sentiments we can see that the review comment message tend to be long because they want to explain why they didn't like the product and others regarding the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-4\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.4. BASE MODEL EVALUATION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "* In this part will be defined several model that'll be used\n",
    "* Base model still using default parameter\n",
    "* Base model will be scored with cross validation & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek the data\n",
    "df_0_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPARE TARGET & FEATURE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined X & Y\n",
    "text = df_0_model['pp2_document_lemmatize']\n",
    "Y = df_0_model['label']\n",
    "\n",
    "# Train Test Split\n",
    "text_train, text_test, Y_train, Y_test = train_test_split(text, Y, stratify = Y, random_state = 8888, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF - IDF**<br><br>\n",
    "\n",
    "Can see the explanation [here](#tf_tf_idf_explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's find out how many features we will generate with TFIDF, by understanding this we know how to adjust `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer \n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# train the vectorizer\n",
    "vectorizer.fit_transform(text)\n",
    "\n",
    "# Let's find out how many features we have\n",
    "print('number of generated features = ', len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll gonna determine optimal `max_features` for `tf_idf`. Because processing all the features is computationally expensive and not efficient. Metrics that we're gonna use to determine optimal features is ROC AUC & F1 Score. The Necessary steps are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a lsit to compile the result\n",
    "bm_f1_score_list = []\n",
    "bm_roc_auc_score_list_c1 = []\n",
    "bm_roc_auc_score_list_c0 = []\n",
    "\n",
    "# Define number of features that we're gonna find out\n",
    "number_of_features = [1, 5, 10, 25, 50, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 6000, None]\n",
    "    \n",
    "# Using forloop to repate benchmarking process\n",
    "for n_features in number_of_features:\n",
    "    \n",
    "    # Define vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features = n_features)\n",
    "\n",
    "    # Fit_transform text data \n",
    "    X_train_tf_idf_bm = vectorizer.fit_transform(text_train)\n",
    "    X_test_tf_idf_bm = vectorizer.transform(text_test)\n",
    "\n",
    "    # We train our model\n",
    "    model_logreg_bm = LogisticRegression(random_state = 8888, solver = 'saga', max_iter = 5000)\n",
    "    model_logreg_bm.fit(X_train_tf_idf_bm, Y_train)\n",
    "\n",
    "    # Make a Prediction\n",
    "    Y_pred_class_tf_idf_bm = model_logreg_bm.predict(X_test_tf_idf_bm)\n",
    "    Y_pred_proba_tf_idf_bm = model_logreg_bm.predict_proba(X_test_tf_idf_bm)\n",
    "\n",
    "    # calculate the result\n",
    "    f1_score_bm = f1_score(Y_test, Y_pred_class_tf_idf_bm)\n",
    "    roc_auc_score_bm_c1 = roc_auc_score(Y_test, Y_pred_proba_tf_idf_bm[:,1])\n",
    "    \n",
    "    # Append the result\n",
    "    bm_f1_score_list.append(f1_score_bm)\n",
    "    bm_roc_auc_score_list_c1.append(roc_auc_score_bm_c1)\n",
    "\n",
    "# Make a dataframe to compile the result\n",
    "bm_max_features_df = pd.DataFrame({'number_of_feaatures' : number_of_features,\n",
    "              'f1_score' : bm_f1_score_list,\n",
    "              'roc_auc_score' : bm_roc_auc_score_list_c1})\n",
    "\n",
    "bm_max_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (17,8))\n",
    "sns.set_style('white')\n",
    "\n",
    "sns.lineplot(x = number_of_features, y = bm_f1_score_list, linewidth = 1.5, color = 'dodgerblue', label = 'f1')\n",
    "sns.scatterplot(x = number_of_features, y = bm_f1_score_list, color = 'dodgerblue')\n",
    "sns.lineplot(x = number_of_features, y = bm_roc_auc_score_list_c1, linewidth = 1.5 ,color = 'darkorange', label = 'ROC_AUC')\n",
    "sns.scatterplot(x = number_of_features, y = bm_roc_auc_score_list_c1, color = 'darkorange')\n",
    "\n",
    "# OtherAttributes\n",
    "plt.title('Metrics by Max Features\\n', size = 18, weight = 'bold')\n",
    "plt.xlabel('Max Features', size = 15)\n",
    "plt.ylabel('Metrics Score', size = 15)\n",
    "plt.legend(loc = 'lower right', prop = {'size' : 13})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEFINE THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model, default model\n",
    "base_logreg = LogisticRegression(random_state = 8888)\n",
    "base_mnb = MultinomialNB()\n",
    "base_xgb = XGBClassifier(use_label_encoder = False, random_state = 8888)\n",
    "\n",
    "# Compile the model in 1 list\n",
    "base_m_compile = [base_logreg, base_mnb, base_xgb]\n",
    "\n",
    "# Classifier name\n",
    "model_name = ['Logreg', 'NB_Multinomial', 'XGB']\n",
    "\n",
    "# Base Model Name\n",
    "base_m_name = ['base_Logreg', 'base_MultNB', 'base_XGB']\n",
    "\n",
    "# Base Model Name + oversampling\n",
    "base_m_oversamp_name = ['oversamp_base_Logreg', 'oversamp_base_MultNB', 'oversamp_base_XGB']\n",
    "\n",
    "# Base Model Name + Udnersampling\n",
    "base_m_undersamp_name = ['undersamp_base_Logreg', 'undersamp_base_MultNB', 'undersamp_base_XGB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING WITH CROSSVALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross validation method\n",
    "cv = StratifiedKFold(n_splits = 10)\n",
    "\n",
    "# Make a list to compile cv result \n",
    "cv_base_model_score_list = []\n",
    "\n",
    "# List of string consist of column name for dataframe that we'll make later\n",
    "cv_col_metrics_ = ['base_cv.f1.mean', 'base_cv.f1.std',\n",
    "                'base_cv.roc_auc.mean', 'base_cv.roc_auc.std',\n",
    "                'base_cv.precision.mean', 'base_cv.precision.std',\n",
    "                'base_cv.recall.mean', 'base_cv.recall.std']\n",
    "\n",
    "# Remember, our text need to be preprocessed. let's define the preprocessing process\n",
    "dtm_tf_idf = TfidfVectorizer(max_features = 2500)\n",
    "\n",
    "# Using forloop to cross validate our models\n",
    "for base_model in base_m_compile:\n",
    "    \n",
    "    # Define the pipeline\n",
    "    model_pipe = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('model', base_model)\n",
    "        ])\n",
    "\n",
    "    # Using cross validate so we can calculate several metrics\n",
    "    base_model_cv = cross_validate(model_pipe, text_train, Y_train, cv = cv, scoring = ['f1', 'roc_auc', 'precision', 'recall'], n_jobs = -1, verbose = True)\n",
    "    \n",
    "    # save cv result in a list\n",
    "    cv_base_model_score_list.append([base_model_cv['test_f1'].mean(), base_model_cv['test_f1'].std(),\n",
    "                             base_model_cv['test_roc_auc'].mean(), base_model_cv['test_roc_auc'].std(),\n",
    "                             base_model_cv['test_precision'].mean(), base_model_cv['test_precision'].std(),\n",
    "                             base_model_cv['test_recall'].mean(), base_model_cv['test_recall'].std()])\n",
    "\n",
    "# Save result in Dataframe\n",
    "df_base_m_cv_score = (pd.DataFrame(cv_base_model_score_list, columns = cv_col_metrics_, index = base_m_name) * 100).round(5)\n",
    "\n",
    "# Print out result in form of dataframe\n",
    "display_side_by_side(df_base_m_cv_score, titles = ['Basic Model | Cross Validation Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING WITH TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list to compile test set score result\n",
    "base_model_test_score_list = []\n",
    "\n",
    "# Prepare Column name for dataframe that show test set score result\n",
    "test_col_metrics_ = ['base_test_f1', 'base_test_roc_auc', 'base_test_precision', 'base_test_recall']\n",
    "\n",
    "# Using forloop to predict test set\n",
    "for modelname, model in zip(base_m_name, base_m_compile):\n",
    "    \n",
    "    # Define Pipeline\n",
    "    model_pipe = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Train Model\n",
    "    model_pipe.fit(text_train, Y_train)\n",
    "    \n",
    "    # Predict proba\n",
    "    Y_pred_proba_ = model_pipe.predict_proba(text_test)[:,1]\n",
    "    \n",
    "    # predict class\n",
    "    Y_pred_class_ = model_pipe.predict(text_test)\n",
    "    \n",
    "    # save model test result in a list\n",
    "    base_model_test_score_list.append([f1_score(Y_test, Y_pred_class_),\n",
    "                                      roc_auc_score(Y_test, Y_pred_proba_),\n",
    "                                      precision_score(Y_test, Y_pred_class_),\n",
    "                                      recall_score(Y_test, Y_pred_class_)])\n",
    "    \n",
    "    # Show classification report.\n",
    "    print('--'*32)\n",
    "    print(f'{modelname}')\n",
    "    print('--'*32)    \n",
    "    print(classification_report(Y_test, Y_pred_class_), '\\n\\n')\n",
    "\n",
    "# Scoring result in form of dataframe\n",
    "df_base_m_test_score = (pd.DataFrame(base_model_test_score_list, index = base_m_name, columns = test_col_metrics_) * 100).round(5)\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(df_base_m_test_score, titles = ['Basic Model | Test Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-5\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.5. BASE MODEL EVALUATION WITH OVERSAMPLING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "* More or less the same with the step at 4.4, but in this section we're gonna add 1 step into the pipeline that is oversampling\n",
    "* Oversampling method that will be used is [**SMOTE**](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING MODEL + OVERSAMPLING(SMOTE) USING CROSS VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the oversampling method\n",
    "smote = SMOTE(random_state = 8888)\n",
    "\n",
    "# List to contain cv scoring for model + oversampling\n",
    "cv_base_model_oversampling_best_score = []\n",
    "\n",
    "# Column identity for dataframe\n",
    "cv_col_metrics_ = ['ovrsm_cv.f1.mean', 'ovrsm_cv.f1.std',\n",
    "                'ovrsm_cv.roc_auc.mean', 'ovrsm_cv.roc_auc.std',\n",
    "                'ovrsm_cv.precision.mean', 'ovrsm_cv.precision.std',\n",
    "                'ovrsm_cv.recall.mean', 'ovrsm_cv.recall.std']\n",
    "\n",
    "# with forloop, score the model + oversampling(SMOTE)\n",
    "for base_model in base_m_compile:\n",
    "       \n",
    "    # Define the pipeline\n",
    "    model_pipe_oversamp = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('oversampling_smote', smote),\n",
    "        ('model', base_model)\n",
    "        ])\n",
    "    \n",
    "    # Cross validation\n",
    "    oversamp_base_model_cv = cross_validate(model_pipe_oversamp, text_train, Y_train, cv = cv, scoring = ['f1', 'roc_auc', 'precision', 'recall'], n_jobs = -1, verbose = True)\n",
    "    cv_base_model_oversampling_best_score.append([oversamp_base_model_cv['test_f1'].mean(), oversamp_base_model_cv['test_f1'].std(),\n",
    "                             oversamp_base_model_cv['test_roc_auc'].mean(), oversamp_base_model_cv['test_roc_auc'].std(),\n",
    "                             oversamp_base_model_cv['test_precision'].mean(), oversamp_base_model_cv['test_precision'].std(),\n",
    "                             oversamp_base_model_cv['test_recall'].mean(), oversamp_base_model_cv['test_recall'].std()])\n",
    "\n",
    "# Scoring result in form of dataframe\n",
    "df_base_m_oversampling_cv_score = (pd.DataFrame(cv_base_model_oversampling_best_score, columns = cv_col_metrics_, index = base_m_oversamp_name) * 100).round(5)\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(df_base_m_oversampling_cv_score, titles = ['Basic Model - Oversampling(SMOTE) | Cross Validation Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING MODEL + OVERSAMPLING(SMOTE) USING TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to contain test score from model + oversampling (SMOTE)\n",
    "base_model_oversmp_test_score_list = []\n",
    "\n",
    "# Column identity for dataframe \n",
    "test_col_metrics_ = ['ovrsm_test_f1', 'ovrsm_test_roc_auc', 'ovrsm_test_precision', 'ovrsm_test_recall']\n",
    "\n",
    "# Using forloop to train model\n",
    "for modelname, model in zip(base_m_name, base_m_compile):\n",
    "    \n",
    "    # Define pipeline\n",
    "    model_pipe_oversamp = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('oversampling_smote', smote),\n",
    "        ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Training model\n",
    "    model_pipe_oversamp.fit(text_train, Y_train)\n",
    "    \n",
    "    # Predict class & proba untuk model + oversampling \n",
    "    Y_pred_oversm_proba_ = model_pipe_oversamp.predict_proba(text_test)[:,1]\n",
    "    Y_pred_oversm_class_ = model_pipe_oversamp.predict(text_test)\n",
    "    \n",
    "    # Scoring\n",
    "    base_model_oversmp_test_score_list.append([f1_score(Y_test, Y_pred_oversm_class_),\n",
    "                                      roc_auc_score(Y_test, Y_pred_oversm_proba_),\n",
    "                                      precision_score(Y_test, Y_pred_oversm_class_),\n",
    "                                      recall_score(Y_test, Y_pred_oversm_class_)])\n",
    "    \n",
    "    # Classification report for each classifier\n",
    "    print('--'*32)\n",
    "    print(f'SMOTE {modelname}')\n",
    "    print('--'*32)    \n",
    "    print(classification_report(Y_test, Y_pred_oversm_class_), '\\n\\n')\n",
    "    \n",
    "# Scoring result in form of dataframe\n",
    "df_base_m_oversmp_test_score = (pd.DataFrame(base_model_oversmp_test_score_list, index = base_m_oversamp_name, columns = test_col_metrics_) * 100).round(5)\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(df_base_m_oversmp_test_score, titles = ['Basic Model - Oversampling(SMOTE) | Test Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-6\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.6. BASE MODEL EVALUATION WITH UNDERSAMPLING</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "* More or less the same with the step at 4.4, but in this section we're gonna add 1 step into the pipeline that is undersampling\n",
    "* Undersampling method that will be used is [**NEARMISS**](https://imbalanced-learn.org/dev/references/generated/imblearn.under_sampling.NearMiss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING MODEL + UNDERSAMPLING(NERMISS) USING CROSS VALIDATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the undersampling method\n",
    "nm = NearMiss()\n",
    "\n",
    "# Empty list to contain cv result for model + undersampling\n",
    "cv_base_model_undersampling_best_score_list = []\n",
    "\n",
    "# Column name for dataframe\n",
    "cv_col_metrics_ = ['undrsm_cv.f1.mean', 'undrsm_cv.f1.std',\n",
    "                'undrsm_cv.roc_auc.mean', 'undrsm_cv.roc_auc.std',\n",
    "                'undrsm_cv.precision.mean', 'undrsm_cv.precision.std',\n",
    "                'undrsm_cv.recall.mean', 'undrsm_cv.recall.std']\n",
    "\n",
    "# Cross validate available model using forloop\n",
    "for base_model in base_m_compile:\n",
    "    \n",
    "    # Define Pipeline   \n",
    "    model_pipe_undersamp = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('undersampling_nm', nm),\n",
    "        ('model', base_model)\n",
    "        ])\n",
    "    \n",
    "    # Cross validation\n",
    "    undersamp_base_model_cv = cross_validate(model_pipe_undersamp, text_train, Y_train, cv = cv, scoring = ['f1', 'roc_auc', 'precision', 'recall'], n_jobs = -1, verbose = True)\n",
    "    \n",
    "    # Append CV Result in the empty list we made before\n",
    "    cv_base_model_undersampling_best_score_list.append([undersamp_base_model_cv['test_f1'].mean(), undersamp_base_model_cv['test_f1'].std(),\n",
    "                             undersamp_base_model_cv['test_roc_auc'].mean(), undersamp_base_model_cv['test_roc_auc'].std(),\n",
    "                             undersamp_base_model_cv['test_precision'].mean(), undersamp_base_model_cv['test_precision'].std(),\n",
    "                             undersamp_base_model_cv['test_recall'].mean(), undersamp_base_model_cv['test_recall'].std()])\n",
    "\n",
    "# Scoring result in form of dataframe\n",
    "df_base_m_undersampling_cv_score = (pd.DataFrame(cv_base_model_undersampling_best_score_list, columns = cv_col_metrics_, index = base_m_undersamp_name) * 100).round(5)\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(df_base_m_undersampling_cv_score, titles = ['Basic Model - UnderSampling(NearMiss) | Cross Validation Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BENCHMARKING MODEL + UNDERSAMPLING(NEARMISS) USING TEST SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list for containing test score result \n",
    "base_model_undersam_test_score_list = []\n",
    "\n",
    "# Column name for dataframe\n",
    "test_col_metrics_ = ['undrsm_test_f1', 'undrsm_test_roc_auc', 'undrsm_test_precision', 'undrsm_test_recall']\n",
    "\n",
    "# Using forloop, training and predict several model.\n",
    "for modelname, model in zip(base_m_name, base_m_compile):\n",
    "    \n",
    "    # Define pipeline\n",
    "    model_pipe_undersamp = Pipeline([\n",
    "        ('preprocessing', dtm_tf_idf),\n",
    "        ('undersampling_nm', nm),\n",
    "        ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Training model\n",
    "    model_pipe_undersamp.fit(text_train, Y_train)\n",
    "    \n",
    "    # Predict class & Predict Proba\n",
    "    Y_pred_undersm_proba_ = model_pipe_undersamp.predict_proba(text_test)[:,1]\n",
    "    Y_pred_undersm_class_ = model_pipe_undersamp.predict(text_test)\n",
    "    \n",
    "    # Scoring\n",
    "    base_model_undersam_test_score_list.append([f1_score(Y_test, Y_pred_undersm_class_),\n",
    "                                      roc_auc_score(Y_test, Y_pred_undersm_proba_),\n",
    "                                      precision_score(Y_test, Y_pred_undersm_class_),\n",
    "                                      recall_score(Y_test, Y_pred_undersm_class_)])\n",
    "    \n",
    "    # Classification report\n",
    "    print('--'*32)\n",
    "    print(f'NearMiss - {modelname}')\n",
    "    print('--'*32)    \n",
    "    print(classification_report(Y_test, Y_pred_undersm_class_), '\\n\\n')\n",
    "    \n",
    "# Scoring result in form of dataframe\n",
    "df_base_m_undersam_test_score = (pd.DataFrame(base_model_undersam_test_score_list, index = base_m_undersamp_name, columns = test_col_metrics_) * 100).round(5)\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(df_base_m_undersam_test_score, titles = ['Basic Model - UnderSampling(NearMiss) | Test Result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-7\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.7. BASE MODEL OUTPUT RECAP</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "* This section is output from step 4.4 - 4.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECAP CROSS VALIDATION OUTPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically data manipulation to recap all the process above in a form of single dataframe\n",
    "\n",
    "# Base model without resampling \n",
    "recap_cv1 = df_base_m_cv_score.T.reset_index().reset_index()\n",
    "recap_cv1.columns = ['index', 'parameter'] + model_name\n",
    "\n",
    "# Base Model + Oversampling (SMOTE) output\n",
    "recap_cv2 = df_base_m_oversampling_cv_score.T.reset_index().reset_index()\n",
    "recap_cv2.columns = ['index', 'parameter'] + model_name\n",
    "\n",
    "# Base Model + Undersampling (NearMiss) output\n",
    "recap_cv3 = df_base_m_undersampling_cv_score.T.reset_index().reset_index()\n",
    "recap_cv3.columns = ['index', 'parameter'] + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe manipulation\n",
    "\n",
    "# Combine 3 dataframe we defined above, into 1 dataframe. \n",
    "recap_cv = pd.concat((recap_cv1, recap_cv2, recap_cv3), axis = 0).sort_values(by = ['index', 'parameter'], ascending = [True, True])\n",
    "\n",
    "# Rename Index\n",
    "metrics_score_recap_cv = recap_cv[recap_cv['parameter'].str.contains('mean')].drop('index', axis = 1).set_index('parameter')\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(metrics_score_recap_cv, titles = ['Basic Model - Cross Validation Result Recap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataframe with title\n",
    "std_recap_cv = recap_cv[recap_cv['parameter'].str.contains('std')].drop('index', axis = 1).set_index('parameter')\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(std_recap_cv, titles = ['Basic Model - Cross Validation StandardDeviation Recap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECAP TEST SET RESULT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically data manipulation to recap all the process above in a form of single dataframe\n",
    "\n",
    "# Dataframe output for base model\n",
    "recap_test1 = df_base_m_test_score.T.reset_index().reset_index()\n",
    "recap_test1.columns = ['index', 'parameter'] + model_name\n",
    "\n",
    "# Dataframe output for Base Model + Oversampling (SMOTE)\n",
    "recap_test2 = df_base_m_oversmp_test_score.T.reset_index().reset_index()\n",
    "recap_test2.columns = ['index', 'parameter'] + model_name\n",
    "\n",
    "# Dataframe output for Base Model + Undersampling (NearMiss)\n",
    "recap_test3 = df_base_m_undersam_test_score.T.reset_index().reset_index()\n",
    "recap_test3.columns = ['index', 'parameter'] + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe manipulation\n",
    "\n",
    "# Combine 3 dataframe we defined above, into 1 dataframe. \n",
    "recap_test = pd.concat((recap_test1, recap_test2, recap_test3), axis = 0).sort_values(by = ['index', 'parameter'], ascending = [True, True])\n",
    "\n",
    "# Rename Index\n",
    "metrics_score_recap_test = recap_test.drop('index', axis = 1).set_index('parameter')\n",
    "\n",
    "# Print dataframe with title\n",
    "display_side_by_side(metrics_score_recap_test, titles = ['Basic Model - Test Result Recap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary consist f1 score (and it's deviation) for base model & resampled data.\n",
    "metrics_plot_f1 = {\n",
    "    # base model\n",
    "    'base_cv_f1_mean' : metrics_score_recap_cv.loc[['base_cv.f1.mean']],\n",
    "    'base_cv_f1_std' : std_recap_cv.loc[['base_cv.f1.std']],\n",
    "    'base_test_f1_mean' : metrics_score_recap_test.loc[['base_test_f1']],\n",
    "    # oversampled model\n",
    "    'base_ovrsmpl_cv_f1_mean' : metrics_score_recap_cv.loc[['ovrsm_cv.f1.mean']],\n",
    "    'base_ovrsmpl_cv_f1_std' : std_recap_cv.loc[['ovrsm_cv.f1.std']],\n",
    "    'base_ovrsmpl_test_f1_mean' : metrics_score_recap_test.loc[['ovrsm_test_f1']],  \n",
    "    # oversampled model\n",
    "    'base_undersmpl_cv_f1_mean' : metrics_score_recap_cv.loc[['undrsm_cv.f1.mean']],\n",
    "    'base_undersmpl_cv_f1_std' : std_recap_cv.loc[['undrsm_cv.f1.std']],\n",
    "    'base_undersmpl_test_f1_mean' : metrics_score_recap_test.loc[['undrsm_test_f1']]\n",
    "}\n",
    "    \n",
    "# Plot metrics\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 3, figsize = (19, 12))\n",
    "\n",
    "# For ax identity\n",
    "i, j = 0, 0\n",
    "\n",
    "# Using forloop, loop the dictionary\n",
    "for title, metric_data in metrics_plot_f1.items():\n",
    "    # Create ax\n",
    "    ax = axs[i, j]\n",
    "    \n",
    "    # If j == 1 means middle graph, we used it for plotting standar deviation\n",
    "    # Differentiate color & xlimit for metric score & standard deviation\n",
    "    if j == 1 :\n",
    "        color_palette = 'Greys_r'\n",
    "        ax.set_xlim(0, 2)\n",
    "    else : \n",
    "        color_palette = 'OrRd_r'\n",
    "        ax.set_xlim(0, 100)\n",
    "        \n",
    "    # Create the graph\n",
    "    ax = sns.barplot(data = metric_data, palette = color_palette, orient = 'h', ax = ax)\n",
    "    \n",
    "    # Graph complimentary\n",
    "    ax.set_title(title, size = 14)\n",
    "    labels(ax)\n",
    "    \n",
    "    # Incrementing the index\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        i += 1\n",
    "\n",
    "fig.suptitle('F1 Score & Standard Deviation from CV & Test Set Result\\n\\n', fontsize = 16, weight = 'bold', x = 0.52)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary consist f1 score (and it's deviation) for base model & resampled data.\n",
    "metrics_plot_roc_auc = {\n",
    "    # Base model\n",
    "    'base_cv_roc_auc_mean' : metrics_score_recap_cv.loc[['base_cv.roc_auc.mean']],\n",
    "    'base_cv_roc_auc_std' : std_recap_cv.loc[['base_cv.roc_auc.std']],\n",
    "    'base_test_roc_auc_mean' : metrics_score_recap_test.loc[['base_test_roc_auc']],\n",
    "    # Oversampled model\n",
    "    'base_ovrsmpl_cv_roc_auc_mean' : metrics_score_recap_cv.loc[['ovrsm_cv.roc_auc.mean']],\n",
    "    'base_ovrsmpl_cv_roc_auc_std' : std_recap_cv.loc[['ovrsm_cv.roc_auc.std']],\n",
    "    'base_ovrsmpl_test_roc_auc_mean' : metrics_score_recap_test.loc[['ovrsm_test_roc_auc']],  \n",
    "    # Oversampled model\n",
    "    'base_undersmpl_cv_roc_auc_mean' : metrics_score_recap_cv.loc[['undrsm_cv.roc_auc.mean']],\n",
    "    'base_undersmpl_cv_roc_auc_std' : std_recap_cv.loc[['undrsm_cv.roc_auc.std']],\n",
    "    'base_undersmpl_test_roc_auc_mean' : metrics_score_recap_test.loc[['undrsm_test_roc_auc']]\n",
    "}\n",
    "    \n",
    "# Plot metrics\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 3, figsize = (19, 12))\n",
    "\n",
    "# For ax identity\n",
    "i, j = 0, 0\n",
    "\n",
    "# Using forloop, loop the dictionary\n",
    "for title, metric_data in metrics_plot_roc_auc.items():\n",
    "    # Create Ax\n",
    "    ax = axs[i, j]\n",
    "    \n",
    "    # If j == 1 means middle graph, we used it for plotting standar deviation\n",
    "    # Differentiate color & xlimit for metric score & standard deviation\n",
    "    if j == 1 :\n",
    "        color_palette = 'Greys_r'\n",
    "        ax.set_xlim(0, 2)\n",
    "    else : \n",
    "        color_palette = 'OrRd_r'\n",
    "        ax.set_xlim(0, 110)\n",
    "        \n",
    "    # Create the graph\n",
    "    ax = sns.barplot(data = metric_data, palette = color_palette, orient = 'h', ax = ax)\n",
    "    \n",
    "    # Graph complimentary\n",
    "    ax.set_title(title, size = 14)\n",
    "    labels(ax)\n",
    "    \n",
    "    # Incrementing the index\n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        j = 0\n",
    "        i += 1\n",
    "\n",
    "fig.suptitle('ROC AUC Score & Standard Deviation from CV & Test Set Result\\n\\n', fontsize = 16, weight = 'bold', x = 0.52)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> * We're gonna use `ROC_AUC` as our metrics because both class are equally important\n",
    "> * We have imbalance dataset (65% positive sentiment, 35% negative sentiment\n",
    "> * Output shows that there's no significant different from the base model and base model with resample method (Handling imbalance)\n",
    "> * Next from all available models, We're gonna choose `Logistic Regression` to continue\n",
    "> * We're not gonna apply resampling methode since they're not giving significant improvement to our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-8\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.8. HYPERPARAMETER TUNING - LOGISTIC REGRESSION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "[**ABOUT LOGISTIC REGRESSION**](https://www.sciencedirect.com/topics/computer-science/logistic-regression)<br><br>\n",
    "Logistic regression is another powerful supervised ML algorithm used for binary classification problems (when target is categorical). The best way to think about logistic regression is that it is a linear regression but for classification problems. Logistic regression essentially uses a logistic function defined below to model a binary output variable (Tolles & Meurer, 2016). The primary difference between linear regression and logistic regression is that logistic regression's range is bounded between 0 and 1. In addition, as opposed to linear regression, logistic regression does not require a linear relationship between inputs and output variables. This is due to applying a nonlinear log transformation to the odds ratio.<br><br>\n",
    "\n",
    "**REFERENCES :**\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "2. https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "3. https://stackoverflow.com/questions/58815016/cross-validating-with-imblearn-pipeline-and-gridsearchcv\n",
    "4. https://www.sciencedirect.com/topics/computer-science/logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRID SEARCH CV LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definding the model that we're gonna used\n",
    "model_logreg = LogisticRegression(random_state = 8888)\n",
    "\n",
    "# Make a pipeline for logistic regression\n",
    "logregpipe = Pipeline([\n",
    "    ('preprocessing', dtm_tf_idf),\n",
    "    ('logreg', model_logreg)\n",
    "    ])  \n",
    "    \n",
    "# Make hyperparameter space for hyperparameter tuning\n",
    "logreg_param_space = {\n",
    "    'logreg__penalty' : ['l2', 'l1', 'elasticnet'],\n",
    "    'logreg__C' : [1, 0.1, 0.05, 0.01, 0.005, 0.001],  \n",
    "    'logreg__random_state' : [8888],\n",
    "    'logreg__solver' : ['liblinear', 'newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "    'logreg__max_iter' : [10, 50, 100, 150, 200],\n",
    "    'logreg__l1_ratio' : [0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "}\n",
    "\n",
    "# Define the gridsearch\n",
    "logreg_grid_search = GridSearchCV(\n",
    "                       logregpipe, \n",
    "                       param_grid = logreg_param_space,\n",
    "                       cv = cv,\n",
    "                       scoring = 'roc_auc',\n",
    "                       n_jobs = -1,\n",
    "                       verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Grid Search CV Logistic Regression\n",
    "logreg_grid_search.fit(text_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best score & best parameter\n",
    "print('Best parameter : ', logreg_grid_search.best_params_, '\\n')\n",
    "print('Best score : ', logreg_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Parameter in DataFrame form\n",
    "logreg_best_param = pd.DataFrame(logreg_grid_search.best_params_, index = [0]).T\n",
    "logreg_best_param.rename(columns = {0 : 'parameter_value'}, inplace = True)\n",
    "display_side_by_side(logreg_best_param, titles = ['Logreg Bestparameter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model before Hyperparameter Tuning\n",
    "logregpipe\n",
    "\n",
    "# Save LogisticRegression Tuned Model in new Variable\n",
    "logreg_best_model = logreg_grid_search.best_estimator_\n",
    "\n",
    "# Showing all available class\n",
    "logreg_best_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the default model\n",
    "logregpipe.fit(text_train, Y_train)\n",
    "\n",
    "# Train Logreg Best Parameter model\n",
    "logreg_best_model.fit(text_train, Y_train)\n",
    "\n",
    "# Get prediction result\n",
    "Y_pred_class_logreg_default = logregpipe.predict(text_test)\n",
    "Y_pred_proba_logreg_default = logregpipe.predict_proba(text_test)\n",
    "\n",
    "# Get prediction & predict proba\n",
    "Y_pred_class_logreg_tuned = logreg_best_model.predict(text_test)\n",
    "Y_pred_proba_logreg_tuned = logreg_best_model.predict_proba(text_test)\n",
    "\n",
    "# Print F1 for default model & tuned model\n",
    "print('f1 LogReg default : ', f1_score(Y_test, Y_pred_class_logreg_default))\n",
    "print('f1 score LogReg tuned : ',f1_score(Y_test, Y_pred_class_logreg_tuned), '\\n')\n",
    "\n",
    "# Calculate the score for ROC AUC Both model (default model & tuned model.)\n",
    "roc_auc_logreg_default = roc_auc_score(Y_test, Y_pred_proba_logreg_default[:,1])\n",
    "roc_auc_logreg_tuned = roc_auc_score(Y_test, Y_pred_proba_logreg_tuned[:,1])\n",
    "\n",
    "# Print ROC AUC for default model & tuned model\n",
    "print('ROC AUC LogReg default : ',roc_auc_logreg_default)\n",
    "print('ROC AUC LogReg tuned : ',roc_auc_logreg_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the output above, we see that there's no significant difference between default model & tuned model. But we can see there's an improvement for the tuned model\n",
    "* We'll continue with the tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA FOR PRECSION RECALL F1 SCORE**\n",
    "<a id=\"4.6.lrprecisionrecall\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics as follows : precision, recall for class 1\n",
    "logreg_precision_c1, logreg_recall_c1, logreg_treshold_c1 = precision_recall_curve(Y_test, Y_pred_proba_logreg_tuned[:,1])\n",
    "\n",
    "# Get metrics as follows : precision, recall for class 0\n",
    "logreg_precision_c0, logreg_recall_c0, logreg_treshold_c0 = precision_recall_curve(Y_test, Y_pred_proba_logreg_tuned[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last element to make precision recall treshold for LogisticRegression Tuned Model\n",
    "# Class 1\n",
    "logreg_precision_e_c1 = np.delete(logreg_precision_c1, -1)\n",
    "logreg_recall_e_c1 = np.delete(logreg_recall_c1, -1) \n",
    "\n",
    "# Make dataframe consist of recall, precision & treshold untuk Logistic Regression Tuned Model Class 1\n",
    "df_PR_logreg_c1 = pd.DataFrame({\n",
    "                        'recall' : logreg_recall_e_c1,\n",
    "                        'precision' : logreg_precision_e_c1,\n",
    "                        'treshold' : logreg_treshold_c1})\n",
    "\n",
    "# Manually count f1 score for class 1\n",
    "df_PR_logreg_c1['f1_score'] = ((2 * df_PR_logreg_c1['precision'] * df_PR_logreg_c1['recall']) / (df_PR_logreg_c1['precision'] + df_PR_logreg_c1['recall']))\n",
    "\n",
    "# Adjust dataframe a bit before showing output\n",
    "df_PR_logreg_c1 = df_PR_logreg_c1[['precision', 'recall', 'f1_score', 'treshold']]\n",
    "\n",
    "# Show Output\n",
    "display_side_by_side(df_PR_logreg_c1.sample(10), titles = ['Precision, Recall & F1 - Score By Threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA FOR ROC AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fpr, tpr dan treshold from our logisticregression Tuned model for class 1 & class 0\n",
    "logreg_fpr_c1, logreg_tpr_c1, logreg_roc_thres_c1 = roc_curve(Y_test, Y_pred_proba_logreg_tuned[:,1])\n",
    "logreg_fpr_c0, logreg_tpr_c0, logreg_roc_thres_c0 = roc_curve(Y_test, Y_pred_proba_logreg_tuned[:,0])\n",
    "\n",
    "# ROC AUC Score\n",
    "logreg_roc_auc_score = roc_auc_score(Y_test, Y_pred_proba_logreg_tuned[:,1])\n",
    "print('LogisticRegression ROC AUC Score = ',logreg_roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT RECAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(Y_test, Y_pred_class_logreg_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision Recall dengan Tresholdnya.\n",
    "plt.figure(figsize = (17,8))\n",
    "sns.set_style('white')\n",
    "# Logistic Regression : Recall Precision F1 Score\n",
    "sns.lineplot(data = df_PR_logreg_c1, x = 'treshold', y = 'f1_score', label = 'LogReg_f1_c1', linewidth = 4, color = 'dodgerblue', linestyle = 'solid')\n",
    "sns.lineplot(data = df_PR_logreg_c1, x = 'treshold', y = 'precision', label = 'LogReg_Precision', linewidth = 3, color = 'skyblue', linestyle = 'dotted', alpha = 0.7)\n",
    "sns.lineplot(data = df_PR_logreg_c1, x = 'treshold', y = 'recall', label = 'LogReg_recall', linewidth = 3, color = 'cadetblue', linestyle = (0, (5, 5)), alpha = 0.5)\n",
    "\n",
    "# OtherAttributes\n",
    "plt.title('Precision, Recall, f1 by Threshold : Tuned LogisticRegression\\n', size = 18, weight = 'bold')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.xticks(ticks = (np.arange(0, 11)/10))\n",
    "plt.yticks(ticks = (np.arange(0, 11)/10))\n",
    "plt.xlabel('Threshold', size = 15)\n",
    "plt.ylabel('Recall / Precision (positive label : 1)', size = 15)\n",
    "plt.legend()\n",
    "\n",
    "# X Line\n",
    "plt.plot([0.05, 0.5], [0.84, 0.84], 'k--', alpha = 0.5)\n",
    "plt.plot([0.05, 0.95], [0.5, 0.5], 'k-.', alpha = 0.5)\n",
    "\n",
    "# Y Line\n",
    "plt.plot([0.5, 0.5], [0.05, 1], 'k-.', alpha = 0.5)\n",
    "plt.plot([0.4, 0.4], [0, 1], 'k--', alpha = 0.15, color = 'k') #0.5 r\n",
    "plt.plot([0.6, 0.6], [0, 1], 'k--', alpha = 0.15, color = 'k')\n",
    "\n",
    "# Text X Line\n",
    "plt.annotate('Basic Thresh 50%', xy = (0.5, 0.5), xytext = (0.05, 0.855), size = 12)\n",
    "plt.annotate('Approx. Existing F1 Score (84%)', xy = (0.5, 0.5), xytext = (0.05, 0.811), size = 12)\n",
    "plt.annotate('50% Metrics Score', xy = (0.5, 0.5), xytext = (0.05, 0.52), size = 12)\n",
    "\n",
    "# Text Y Line\n",
    "plt.annotate('Basic Threshold : 50%', xy = (0.5, 0.5), xytext = (0.487, 0.07), size = 12, rotation = 90)\n",
    "plt.annotate('Thresh. : 40%', xy = (0.5, 0.5), xytext = (0.387, 0.07), size = 12, rotation = 90, color = 'black', alpha = 0.2)\n",
    "plt.annotate('Thresh. : 60%', xy = (0.5, 0.5), xytext = (0.59, 0.07), size = 12, rotation = 90, color = 'black', alpha = 0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(loc = 'upper right', prop = {'size' : 13})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remember that both class is important, hence F1 score is metrics that we choose ($FP$ & $FN$ is equally important)\n",
    "* From above graph we use $(50 \\%) $ threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(logreg_fpr_c1, logreg_tpr_c1,\n",
    "         label = f'Logistic Regression Tuned, AUC = {logreg_roc_auc_score.round(7)}', \n",
    "         linewidth = 3,\n",
    "         linestyle = 'solid', \n",
    "         color = 'dodgerblue')\n",
    "\n",
    "# Graph Complementary\n",
    "plt.xticks(ticks = (np.arange(0, 11)/10))\n",
    "plt.yticks(ticks = (np.arange(0, 11)/10))\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth = 2, linestyle = 'dashed', alpha = 0.7)\n",
    "plt.axis([-0.01, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize = 16)\n",
    "plt.ylabel('True Positive Rate', fontsize = 16)\n",
    "plt.annotate('''\n",
    "Minimum ROC Score of 50% \n",
    "(This is the minimum score to get) \n",
    "\n",
    "If you get 50% score, \n",
    "  - Do model improvement or, \n",
    "  - Change classifier instead''',\n",
    "             xy = (0.5, 0.5), xytext = (0.6, 0.2), size = 12, \n",
    "             arrowprops = dict(facecolor = '#6E726D', shrink = 0.05),)\n",
    "\n",
    "plt.title('ROC AUC : Tuned LogisticRegression\\n', size = 18, weight = 'bold')\n",
    "plt.xlabel('False Positive Rate', size = 15)\n",
    "plt.ylabel('True Positive Rate', size = 15)\n",
    "plt.legend(loc = 'lower right', prop = {'size' : 12 })\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "> * From above graph we can understand that our classifier has good capabilities to differentiate between class 0 (`PositiveSentiment`) and class 1 (`NegativeSentiment`). We can see that capabilities from the given ROC AUC Score $(93.487 \\%)$\n",
    "> * The model that we're gonna use to classifiy negative & positive sentiment is LogisticRegression\n",
    "> * We try handling imbalance with resample method (oversampling & undersampling), both of it not giving any significance difference from basic model\n",
    "> * Threshold that we're gonna use is  $(50 \\%) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-9\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.9. FEATURE IMPORTANCE</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list consist of featurenames, abs(coef value), coef value\n",
    "feature_coef = [(logreg_best_model[0].get_feature_names_out()[i],\n",
    "  logreg_best_model[1].coef_.flatten()[i]) for i in range(len(logreg_best_model[0].get_feature_names_out()))]\n",
    "\n",
    "# Sort coef based on absolute value\n",
    "sorted_features_coef_pos = sorted(feature_coef, key = lambda x : x[1], reverse = False)\n",
    "sorted_features_coef_neg = sorted(feature_coef, key = lambda x : x[1], reverse = True)\n",
    "\n",
    "# We determine that we can show 10 of most important features of each positive sentiment & negative sentiment\n",
    "shown_feature_pos = sorted_features_coef_pos[:10]\n",
    "shown_feature_neg = sorted_features_coef_neg[:10]\n",
    "\n",
    "# Make it in a form of dataframe\n",
    "df_feature_importance_pos = pd.DataFrame(shown_feature_pos, columns = ['feature', 'coef'])\n",
    "df_feature_importance_neg = pd.DataFrame(shown_feature_neg, columns = ['feature', 'coef'])\n",
    "\n",
    "# Show the output\n",
    "display_side_by_side(df_feature_importance_pos, df_feature_importance_neg, titles = ['coeff. pos.sentiment', 'coeff. neg.sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary for coeff pos & neg. sentiment\n",
    "feature_importance_sentiment = {\n",
    "    # base model\n",
    "    'coeff.pos.sentiment' : df_feature_importance_pos,\n",
    "    'coeff.neg.sentiment' : df_feature_importance_neg}\n",
    "\n",
    "# Plot metrics\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (19, 12))\n",
    "\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "\n",
    "sns.barplot(data = df_feature_importance_pos, x = 'coef', y = 'feature', palette = 'Blues_r', orient = 'h', ax = ax1)\n",
    "sns.barplot(data = df_feature_importance_neg, x = 'coef', y = 'feature', palette = 'OrRd_r', orient = 'h', ax = ax2)\n",
    "\n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "ax1.set_xlabel('coefficient', fontsize = 16)\n",
    "ax1.set_ylabel('features', fontsize = 16)\n",
    "ax2.set_xlabel('coefficient', fontsize = 16)\n",
    "ax2.set_ylabel('features', fontsize = 16)\n",
    "\n",
    "\n",
    "fig.suptitle('Feature Importance\\n', fontsize = 20, weight = 'bold', x = 0.54)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMARKS**\n",
    "* Above graphs shown 10 most important words that differentiate positive & negative sentiments\n",
    "* We can see that `dissatisfied` is in the top 10 for words that describe negative sentiment. The word comes from the process of converting emoji to text. This point wants to explain that emojis also need to be considered for processing in text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-10\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.10. IMPLEMENTATION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to visualize new sentiment\n",
    "def sentiment_analysis(review_comment, best_model) :\n",
    "    # Must have package\n",
    "    import googletrans\n",
    "    from googletrans import Translator\n",
    "    \n",
    "    # Define the translator\n",
    "    translator = Translator()\n",
    "\n",
    "    # Translation Process\n",
    "    translation = translator.translate(review_comment, dest = 'en')\n",
    "    translated_document = translation.text\n",
    "    \n",
    "    # Standardized the text 1 : Convert the emoji\n",
    "    translated_document_pp = replace_emoticon_for_output(translated_document)\n",
    "\n",
    "    # Print the reviewcomment message & translation if it's not in english.\n",
    "    print('input          : ', review_comment)\n",
    "    print('translated doc : ', translated_document_pp)\n",
    "    \n",
    "    # Standardized the text 2\n",
    "    # lowercase, removecontraction, remove number, remove punctuation, removewhitespace, removestopwords, lemmatize\n",
    "    translated_document_pp = to_lower(translated_document_pp)\n",
    "    translated_document_pp = main_contraction(translated_document_pp)\n",
    "    translated_document_pp = remove_numbers(translated_document_pp)\n",
    "    translated_document_pp = remove_punct2(translated_document_pp)\n",
    "    translated_document_pp = to_strip(translated_document_pp)\n",
    "    translated_document_pp = remove_stopwords(translated_document_pp)\n",
    "    translated_document_pp = [lemmatize(translated_document_pp)]\n",
    "       \n",
    "    # Predict Sentiment\n",
    "    class_pred = best_model.predict(translated_document_pp)\n",
    "    pred_proba = best_model.predict_proba(translated_document_pp)\n",
    "    \n",
    "    # Visualize the output\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    if class_pred[0] == 1:\n",
    "        sentiment = f'Negative'\n",
    "        class_proba = round(pred_proba[0][1] * 100)\n",
    "        color = 'crimson'\n",
    "    else:\n",
    "        sentiment = 'Positive'\n",
    "        class_proba = round(pred_proba[0][0] * 100)\n",
    "        color = 'seagreen'\n",
    "    ax.text(0.5, 0.5, sentiment, fontsize = 50, ha = 'center', color = color)\n",
    "    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize = 14, ha = 'center')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Sentiment Analysis', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several review comment message in portugese for testing.\n",
    "review_comment_input_1 = 'Péssimo produto!😡😡 Não compro nessa loja, a entrega atrasou e custou muito dinheiro!'\n",
    "review_comment_input_2 = 'Adoreie realmente cumpriu as expectativas. 💟 Comprei por Um valor barato. Maravilhoso'\n",
    "review_comment_input_3 = 'Não sei gostei do produto. O custo foi barato mas veio com defeito. Se der sorte, vale a pena'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(review_comment_input_1, best_model = logreg_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(review_comment_input_2, best_model = logreg_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(review_comment_input_3, best_model = logreg_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_comment_4 = 'sepatunya nyaman dipakai, ringan, empuk Dan warnanya sesuai dgn yg diharapkan. sellernya ramah, komunikatif & fast respon. pengiriman cepat. recommended pokoknya '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(review_comment_4, best_model = logreg_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_comment_5 = 'produk ini cukup baik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(review_comment_5, best_model = logreg_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4-11\"></a>\n",
    "<font color=\"lightseagreen\" size=+2><b>4.11. LEARNING CURVE</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model, default model\n",
    "base_logreg_lr = LogisticRegression(random_state = 8888, solver = 'saga')\n",
    "base_mnb_lr = MultinomialNB()\n",
    "base_xgb_lr = XGBClassifier(use_label_encoder = False, random_state = 8888)\n",
    "\n",
    "# define the vectorizer\n",
    "dtm_tf_idf = TfidfVectorizer(max_features = 2500)\n",
    "\n",
    "# define the data\n",
    "text_df = dtm_tf_idf.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize = (20, 15))\n",
    "\n",
    "title = 'Learning Curves (Logistic Regression)'\n",
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 8888)\n",
    "estimator = base_logreg_lr\n",
    "plot_learning_curve(\n",
    "    estimator, title, text_df, Y, axes = axes[:, 0], ylim = (0.7, 1.01), cv = cv, n_jobs = -1\n",
    ")\n",
    "\n",
    "title = 'Learning Curves (Multinomial Naive Bayes)'\n",
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 8888)\n",
    "estimator = base_mnb_lr\n",
    "plot_learning_curve(\n",
    "    estimator, title, text_df, Y, axes = axes[:, 1], ylim = (0.7, 1.01), cv = cv, n_jobs = -1\n",
    ")\n",
    "\n",
    "title = 'Learning Curves (XGBoost)'\n",
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 8888)\n",
    "estimator = base_xgb_lr\n",
    "plot_learning_curve(\n",
    "    estimator, title, text_df, Y, axes = axes[:, 2], ylim = (0.7, 1.01), cv = cv, n_jobs = -1\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<font color=\"lightseagreen\" size=+3><b>5. CONCLUSION & RECOMMENDATION</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \n",
    "style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightseagreen\" size=+2><b>BUSINESS</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERALL BUSINESS**\n",
    "- Positive transaction trend in Olist can indicate several things as follows: \n",
    "    - People like using the platform\n",
    "    - More and more people start using the platform\n",
    "- Credit card is the most used payment method in the Olist\n",
    "- The southern part of Brazil has more order density then all the other parts. This suggests that the Brazillian Ecommeerce could prioritize expansion or marketing expense on the northern part of Brazil\n",
    "- Almost all customer states have a good proportion of positive sentiments $(>60\\%)$\n",
    "\n",
    "**REVIEW COMMENT MESSAGE**\n",
    "- Over $58.70\\%$ orders received comments while the rest do not $(41.30\\%)$. After statistical test, it is inferred that customers that do not comment might have a give statistically different review score than those who do (Potentially higher score)\n",
    "- Negative sentiment comment tend to be longer than the positive one. This is normal because when the product is already good there's no complain\n",
    "- The insight ngrams analysis suggests that the most important factor in customer sentiment is order delivery timeliness. Both positive and negative reviews are centered around the delivery. The next potential factor is product quality unfortunately cannot be analyzed using the current data\n",
    "\n",
    "**ORDERS**\n",
    "- Overall, most orders have a review score of 5 $(57.78\\%)$, followed by 4 $(19.29\\%)$ and closely by 1 $(11.51\\%)$. Meanwhile, for review score of 2 and 3 constitute no more than $12\\%$. This data suggests that the customers tend to skip the in between score (2 and 3) right to the extremes if the order performance is poor\n",
    "- There seems to be no difference in customer sentiments in terms of delivery path (i.e. distance). This suggests that in the end, distance may not play part in the overall customer sentiment, as long as the delivery process is under control\n",
    "- $90.30 \\%$ of orders is on time. Overall, the delivery timeliness performance of Olist is good, with items delivered $12$ days **before** the estimated time\n",
    "- After statistical test, delivery timeliness is the biggest factor for determining customer review score and overall satisfaction\n",
    "\n",
    "**CUSTOMERS & PRODUCTS**\n",
    "- Most customers are satisfied shopping at the Olist because they like the product and the product itself arrived on time or even before the estimated arrival and dislike out of time orders\n",
    "- Product quality also play an imporant role in customer sentiment next to delivery timeliness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECOMMENDATION FOR BUSINESS**\n",
    "- Olist could expand further to the northern part of Brazil since it is largely still available for expansion\n",
    "- The business should maintain and improve upon the order delivery, may it be in delivery services or logistics, since it is the most important factor in customer sentiment\n",
    "- Olist should implement customer service to accomodate for complain management. This way, the severity of negative sentiment cases could be minimzed\n",
    "- Olist should add delivery service/logistics as a feature in their database. This is a crucial feature that is directly related to order delivery and as such can unveil important business insights\n",
    "- Olist could consider to incentivize giving reviews, since more reviews mean more insights and improvements to be gained. Over $40\\%$ orders are stll without comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"lightseagreen\" size=+2><b>MACHINE LEARNING</b></font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY SENTIMENT ANALYSIS**\n",
    "- Olist now understand customer response to the seller in the platform.\n",
    "    - By understanding positive response, Olist can determine what are the things that can be maintained to preserve the positive response. \n",
    "    - By understanding the negative response, they can determine what the things need to be improved on.\n",
    "\n",
    "- No need to classify the review comment message manually. Can use the model that we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUR MACHINE LEARNING**\n",
    "* Based on the consequences, we want to mainly Maximize 𝑇𝑃 and 𝑇𝑁, Minimize 𝐹𝑃 and 𝐹𝑁 equally. This approach would give us a model that could prioritize correctly predicting both positive and negative sentiments, hence the metrics we're gonna use is `ROC_AUC` (secondary metrics `f1`)\n",
    "* Model that we use to classify the sentiment is LogisticRegression that has been tuned \n",
    "* Model performance can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confussion Matrix\n",
    "sns.set(font_scale = 1.4)\n",
    "fig, ax1 = plt.subplots(1, 1,figsize = (8,8))\n",
    "fig.suptitle(\"Confussion Matrix for Sentiment Analysis | Tuned LogReg\", size = 16, weight = 'bold')\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, Y_pred_class_logreg_tuned, cmap = \"OrRd\", ax = ax1)\n",
    "plt.grid(visible = False)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\\nClassification Report Tuned Logistic Regression\\n\")\n",
    "print(classification_report(Y_test, Y_pred_class_logreg_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC AUC LogReg tuned : ',roc_auc_logreg_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From confusion matrix & metrics score above, indicating that our model can perform well (can be trusted & reliable) on the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECOMMENDATION FOR MACHINE LEARNING**\n",
    "- Can try another algorithm to see if there’s any algorithm that can performed better than the models that area available in the notebook\n",
    "- Try labelling using VADER to label our review comment message\n",
    "- Trace the data that is still wrongly predicted to find out the cause of the wrong prediction and become the basis for future model improvement\n",
    "- With the same approach in the notebook, we can understand competitor pros & cons. By understanding Olist competitor & learn from it, Olist can keep improve their services"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "df908bfd2693f818c6962bdf1a1f77778fd7094814c18f294566e131c26403d9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
